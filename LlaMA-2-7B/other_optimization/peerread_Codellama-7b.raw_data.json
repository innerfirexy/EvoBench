{
    "original": [
        "This paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nWikipedia).  (Its title and introduction are a little overblown/misleading,\nsince there is a lot more to bridging text and knowledge than the EDL task, but\nEDL is a core part of the overall task nonetheles",
        "This paper presents several weakly supervised methods for developing NERs. The\nmethods rely on some form of projection from English into another language. The\noverall approach is not new and the individual methods proposed are\nimprovements of existing methods. For an ACL paper I would have expected more\nnovel approaches.\n\nOne of the contributions o",
        "This paper describes a rule based approach to time expression extraction. Its\nkey insights are time expressions typically are short and contain at least 1\ntime token. It first recognizes the time token through a combination of\ndictionary lookup, regular expression match with POS tagging information. It\nthen expands the time segment from either dire",
        "This paper presents a neural network-based framework for dialogue state\ntracking.\nThe main contribution of this work is on learning representations of user\nutterances, system outputs, and also ontology entries, all of which are based\non pre-trained word vectors.\nParticularly for the utterance representation, the authors compared two\ndifferent neura",
        "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of t",
        "This paper presents a new dataset with annotations of products coming from\nonline cybercrime forums. The paper is clear and well-written and the\nexperiments are good. Every hypothesis is tested and compared to each other.\n\nHowever, I do have some concerns about the paper:\n\n1. The authors took the liberty to change the font size and the line spacing",
        "This paper introduces UCCA as a target representation for semantic parsing and\nalso describes a quite successful transition-based parser for inference into\nthat representation. I liked this paper a lot. I believe there is a lot of\nvalue simply in the introduction of UCCA (not new, but I believe relatively new\nto this community), which has the poten",
        "This paper proposed a macro discourse structure scheme. The authors carried out\na pilot study annotating a corpus consisting of 97 news articles from Chinese\ntreebank 8.0. They then built a model to recognize the primary-secondary\nrelations and 5 discourse relations (joint, elaboration, sequence, background,\ncause-result) in this corpus.\n\nThe paper",
        "This paper proposes a framework for evaluation of word embeddings based on data\nefficiency and simple supervised tasks. The main motivation is that word\nembeddings are generally used in a transfer learning setting, where evaluation\nis done based on how faster is to train a target model. The approach uses a set\nof simple tasks evaluated in a supervi",
        "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on the analogy task and for the general\nsuperiority of additive composition models. It also establishes a link between\nskip-gram and Sufficient Dimensionality Reduction.\n\nI liked the focus of this paper on explaining the properties of sk",
        "This paper presents a neural sequence-to-sequence model for encoding dialog\ncontexts followed by decoding system responses in open-domain conversations.\nThe authors introduced conditional variational autoencoder (CVAE) which is a\ndeep neural network-based generative model to learn the latent variables for\ndescribing responses conditioning dialog co",
        "This paper compares different ways of inducing embeddings for the task of\npolarity classification. The authors focus on different types of corpora and\nfind that not necessarily the largest corpus provides the most appropriate\nembeddings for their particular task but it is more effective to consider a\ncorpus (or subcorpus) in which a higher concentr",
        "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attention-based\nmatching, and aggregation. The model has two variants, one based on TreeRNNs\nand the other based on sequential BiLSTMs. The sequential model outperforms all\npublished results, and an ensemble with the",
        "This paper proposes a supervised deep learning model for event factuality\nidentification.  The empirical results show that the model outperforms\nstate-of-the-art systems on the FactBank corpus, particularly in three classes\n(CT-, PR+ and PS+).  The main contribution of the paper is the proposal of an\nattention-based two-step deep neural model for e",
        "This paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurre",
        "This paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively and quantitatively analyze the cold-start problem.\nThey observe that there is no enough prior data from a new user in this\nrealistic scenario. The traditional features fail to help to identify review\nspam. Instead, they turn to rely on the abund",
        "This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich ones (e.g.\nSpanish) in a two-step process. First, a system translates into a simplified\nversion of the target language. Second, a system chooses morphological features\nfor each generated target word, and inflects t",
        "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the late",
        "This paper describes interesting and ambitious work: the automated conversion\nof Universal Dependency grammar structures into [what the paper calls] semantic\nlogical form representations.  In essence, each UD construct is assigned a\ntarget construction in logical form, and a procedure is defined to effect the\nconversion, working \u2018inside-out\u2019 using ",
        "This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modular sense selection and representation process. The learning is\nachieved by a message passing scheme between the two modules that is cast as a\nreinforcement learning problem by the authors.\n\n- Strengths:\n\nThe paper is generally well written, presents most of",
        "This paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an efficient A* decoding algorithm.\nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more\nexpressive global parsing model, presumably because this factorization makes\nlearning easier. It's great that they also repor",
        "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to an artist whilst being\nfluent and co-herent. The paper is well written and the motivation for the\nmetrics are well explained.  \n\nThe authors describe both hand annotated metrics (fluency, co-herence and\nmatch) and ",
        "This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization. A RNN with attention\nmechanism is employed to compute a representation of text. The experiments on\nvarious of dataset shows the effectiveness of the proposed method. Below are my\ncomments:\n\n(1) From Table 2, it shows t",
        "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone ",
        "This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as corresponding logical forms and language\ndescriptions.\nThe goal seems to be to have a method where the complexity of pictures and\ncorresponding desciptions can be controlled and parametrized. \n\n - The biggest downside seems to be that the maxima",
        "This paper propose a general framework for analyzing similarities and\ndifferences in term meaning and representation in different contexts.\n\n- Strengths:\n* The framework proposed in this paper is generalizable and can be applied to\ndifferent applications, and accommodate difference notation of context,\ndifferent similarity functions, different type",
        "This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,\nSVD) to include ngram-ngram cooccurance statistics. To deal with the large\ncomputational costs of storing such expensive matrices, the authors propose an\nalgorithm that uses two different strategies to collect counts.  \n\n- Strengths:\n\n* The proposed work seems like a na",
        "This paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relations, similar to the\nanalogical reasoning task of Mikolov et al. (2013): Given an expression of the\nform \u201cX is for France what London is for the UK\u201d, X can be approximated by\nthe simple vector arithmetic operation L",
        "This paper describes a straightforward extension to left-to-right beam search\nin order to allow it to incorporate lexical constraints in the form of word\nsequences that must appear in MT output. This algorithm is shown to be\neffective for interactive translation and domain adaptation.\n\nAlthough the proposed extension is very simple, I think the pap",
        "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n\u201cprogrammer\u201d) which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a \u2018key variable\u2019 memory\ncomponent which stores (a) entities in the questio",
        "This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution task by using these inventories to filter\ncandidates. To do so, the authors first propose a metric to measure the mutual\nsubstitutability of sense inventories with human judgments for the lexsub task,\nand empirically measure the substitutabi",
        "This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the corpus:\n\n1) Student Revision Behavior Analysis and 2) Automatic Revision Identification\n\nThe latter is essentially a text classification task using an SVM classifier\nand a variety of features. The authors state that the corpus will be freely",
        "This paper presents a dialogue agent where the belief tracker and the dialogue\nmanager are jointly optimised using the reinforce algorithm. It learns from\ninteraction with a user simulator. There are two training phases. The first is\nan imitation learning phase where the system is initialised using supervising\nlearning from a rule-based model. Then",
        "This paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nstate-of-the-art system (Zhou and Xu, 2015) with recent best practices for\ninitialization and regularization in the deep learning literature.\nThe model gives a 10% relative error reduction which is a big gain o",
        "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something called the major system. In the major system, each digit\nis mapped to one or more characters representing consonantal phonemes; the\npossible mappings between digit and phoneme are predefined. The output of an\nencoding is typically a sequence of words co",
        "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop\nreasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but\nalso have major concerns to g",
        "This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the proposed model outperforms other\nbaselines if supervised data is available.\n\n- Strengths:\nThe paper is well-organized and easy to follow (the intuition of the proposed\nmethod is clear). It includes enough details to replicate experim",
        "This paper continues the line of work for applying word embeddings for the\nproblem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015;\n\u00dcst\u00fcn & Can, 2016). The proposed method, MORSE, applies a local optimization\nfor segmentation of each word, based on a set of orthographic and semantic\nrules and a few heuristic threshold values as",
        "This paper proposes a simple attention-based RNN model for generating SQL\nqueries from natural language without any intermediate representation. Towards\nthis end they employ a data augmentation approach where more data is\niteratively collected from crowd annotation, based on user feedback on how well\nthe SQL queries produced by the model do. Result",
        "This paper presents a graph-based approach for producing sense-disambiguated\nsynonym sets from a collection of undisambiguated synonym sets.  The authors\nevaluate their approach by inducing these synonym sets from Wiktionary and from\na collection of Russian dictionaries, and then comparing pairwise synonymy\nrelations (using precision, recall, and F",
        "This paper proposes a joint model of salient phrase selection and discourse\nrelation prediction in spoken meeting. Experiments using meeting corpora show\nthat the proposed model has higher performance than the SVM-based classifier.\n\n- Strengths:\nThe paper is written to be easy to read. Technical details are described fully,\nand high performance is ",
        "This paper proposes a method for building dialogue agents involved in a\nsymmetric collaborative task, in which the agents need to strategically\ncommunicate to achieve a common goal.  \n\nI do like this paper.  I am very interested in how much data-driven techniques\ncan be used for dialogue management.  However, I am concerned that the approach\nthat t",
        "This paper proposes an approach for classifying literal and metaphoric\nadjective-noun pairs. The authors create a word-context matrix for adjectives\nand nouns where each element of the matrix is the PMI score. They then use\ndifferent methods for selecting dimensions of this matrix to represent each\nnoun/adjective as a vector. The geometric properti",
        "This paper proposes a novel strategy for zero-resource translation where\n(source, pivot) and (pivot, target) parallel corpora are available. A teacher\nmodel for p(target|pivot) is first trained on the (pivot, target) corpus, then\na student model for p(target|source) is trained to minimize relative entropy\nwith respect to the teacher on the (source,",
        "This paper considers the problem of KB completion and proposes ITransF for this\npurpose. Unlike STransE that assigns each relation an independent matrix, this\npaper proposes to share the parameters between different relations. A model is\nproposed where a tensor D is constructed that contains various relational\nmatrices as its slices and a selection",
        "This paper presents a method for translating natural language descriptions into\nsource code via a model constrained by the grammar of the programming language\nof the source code.  I liked this paper - it's well written, addresses a hard\nand interesting problem by taking advantage of inherent constraints, and shows\nsignificant performance improvemen",
        "This paper focuses on interpreting sarcasm written in Twitter identifying\nsentiment words and then using a machine translation engine to find an\nequivalent not sarcastic tweet. \n\nEDIT: Thank you for your answers, I appreaciate it. I added one line commenting\nabout it.\n\n- Strengths:\n\nAmong the positive aspects of your work, I would like to mention t",
        "This paper describes a system to assist written test scoring.\n\n- Strengths:\nThe paper represents an application of an interesting NLP problem --\nrecognizing textual entailment -- to an important task -- written test scoring.\n\n- Weaknesses:\nThere isn't anything novel in the paper. It consist of an application of an\nexisting technology to a known pro",
        "This paper introduces new configurations and training objectives for neural\nsequence models in a multi-task setting. As the authors describe well, the\nmulti-task setting is important because some tasks have shared information\nand in some scenarios learning many tasks can improve overall performance.\n\nThe methods section is relatively clear and logi",
        "This paper proposed a new phrasal RNN architecture for sequence to sequence\ngeneration. They have evaluated their architecture based on (i) the language\nmodelling test evaluated on PTB and FBIS and (ii) Chinese-English machine\ntranslation task on NIST MT02-08 evaluation sets. The phrasal RNN (pRNN)\narchitecture is achieved by generating subnetworks",
        "This paper proposes new prediction models for Japanese SRL task by adopting the\nEnglish state-of-the-art model of (Zhou and Xu, 2015).\nThe authors also extend the model by applying the framework of Grid-RNNs in\norder to handle the interactions between the arguments of multiple predicates.\n\nThe evaluation is performed on the well-known benchmark dat",
        "This paper develops an LSTM-based model for classifying connective uses for\nwhether they indicate that a causal relation was intended. The guiding idea is\nthat the expression of causal relations is extremely diverse and thus not\namenable to syntactic treatment, and that the more abstract representations\ndelivered by neural models are therefore more",
        "This paper proposes a method for recognizing lexical entailment (specifically,\nhypernymy) in context. The proposed method represents each context by\naveraging, min-pooling, and max-pooling its word embeddings. These\nrepresentations are combined with the target word's embedding via element-wise\nmultiplication. The in-context representation of the le",
        "This paper proposes a method for evaluating topic quality based on using word\nembeddings to calculate similarity (either directly or indirectly via matrix\nfactorisation), achieving impressive results over standard datasets.\n\nThe proposed method represents a natural but important next step in the\nevolutionary path of research on topic evaluation. Th",
        "This paper proposed a very interesting idea of using cognitive features for\nsentiment analysis and sarcasm detection. More specifically, the eye-movement\npatterns of human annotators are recorded to derive a new set of features. The\nauthors claim that this is the first work to include cognitive features into\nthe NLP community. \n\nStrength: \n1. The p",
        "This paper proposes a neural-styled topic model, extending the objective of\nword2vec to also learn document embeddings, which it then constrains through\nsparsification, hence mimicking the output of a topic model.\n\nI really liked the model that the authors proposed, and found the examples\npresented by the authors to be highly promising. What was re",
        "This paper describes four methods of obtaining multilingual word embeddings and\na modified QVEC metric for evaluating the efficacy of these embeddings. The\nembedding methods are: \n\n(1) multiCluster : Uses a dictionary to map words to multilingual clusters.\nCluster embeddings are then obtained which serve as embeddings for the words\nthat reside in e",
        "This paper proposes a method for discovering correspondences between languages\nbased on MDL. The author model correspondences between words sharing the same\nmeaning in a number of Slavic languages. They develop codes for rules that\nmatch substrings in two or more languages and formulate an MDL objective that\nbalances the description of the model an",
        "This paper proposes an approach for multi-lingual named entity recognition\nusing features from Wikipedia. By relying on a cross-lingual Wikifier, it\nidentifies English Wikipedia articles for phrases in a target language and uses\nfeatures based on the wikipedia entry. Experiments show that this new feature\nhelps not only in the monolingual case, but",
        "This paper proposes the new (to my knowledge) step of proposing to treat a\nnumber of sentence pair scoring tasks (e.g. Answer Set Scoring, RTE,\nParaphrasing,\namong others) as instances of a more general task of understanding semantic\nrelations\nbetween two sentences. Furthermore, they investigate the potential of learning\ngenerally-\napplicable neura",
        "This paper presents a transition-based graph parser able to cope with the rich\nrepresentations of a semantico-cognitive annotation scheme, instantiated in the\nUCCA corpora. The authors start first by exposing what, according to them,\nshould cover a semantic-based annotation scheme: (i) being graph-based\n(possibility for a token/node of having multi",
        "This paper presents a Stack LSTM parser based on the work of Henderson et al.\n(2008, 2013) on joint syntactic/semantic transition-based parsing and Dyer et\nal. (2015) on stack LSTM syntactic parsing. The use of the transition system\nfrom the former and the stack LSTM from the latter shows interesting results\ncompared to the joint systems on the CoN",
        "This paper investigates three simple weight-pruning techniques for NMT, and\nshows that pruning weights based on magnitude works best, and that retraining\nafter pruning can recover original performance, even with fairly severe\npruning.\n\nThe main strength of paper is that the technique is very straightforward and\nthe results are good. It\u00e2\u0080\u0099s also cle",
        "This paper presents results on the UD treebanks to test delexicalized transfer\nparsers and an unsupervised parser which is enriched with external\nprobabilities.\n\nThe paper is interesting, but I think it could be improved further.\n\n(5.2) \"McDonald et al. (2011) presented 61.7% of averaged accuracy over 8\nlanguages. On the same languages, our transfe",
        "This paper presents an approach to tag word senses with temporal information\n(past, present, future or atemporal). They model the problem using a\ngraph-based semi-supervised classification algorithm that allows to combine\nitem specific information - such as the presence of some temporal indicators in\nthe glosses - and the structure of Wordnet - tha",
        "This paper models event linking using CNNs. Given event mentions, the authors\ngenerate vector representations based on word embeddings passed through a CNN\nand followed by max-pooling. They also concatenate the resulting\nrepresentations with several word embeddings around the mention. Together with\ncertain pairwise features, they produce a vector o",
        "This paper describes a new deterministic dependency parsing algorithm and\nanalyses its behaviour across a range of languages.\nThe core of the algorithm is a set of rules defining permitted dependencies\nbased on POS tags.\nThe algorithm starts by ranking words using a slightly biased PageRank over a\ngraph with edges defined by the permitted dependenc",
        "This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on r",
        "This paper extends an approach to rate-distortion optimization to deep encoders and decoders, and from a simple entropy encoding scheme to adaptive entropy coding. In addition, the paper discusses the approach\u2019s relationship to variational autoencoders.\n\nGiven that the approach to rate-distortion optimization has already been published, the novelty",
        "This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points.\n\nPros:\n\n- An interesting and feasible approach to me",
        "This paper was submitted to arXiv last week:\n\n",
        "This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. \n\nWith the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantl",
        "This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the ",
        "This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising",
        "This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-writ",
        "This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems ",
        "This paper proposes to investigate attention transfers between a teacher and a student network. \n\nAttention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.\nAuthors define several activat",
        "This paper investigates the benefits of visual servoing using a learned\nvisual representation. The authors  propose to first learn an action-conditional\nbilinear model of the visual features (obtained from a pre-trained VGG net) from\nwhich a policy can be derived using a linearization of the dynamics. A multi-scale,\nmulti-channel and locally-connec",
        "This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of s",
        "This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines. \nThe results are a 10% improvements in ResNet-like and ImageNe",
        "This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a metho",
        "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pix",
        "This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and Alex",
        "This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The ex",
        "This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the",
        "This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representation",
        "This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itsel",
        "This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different vi",
        "This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is",
        "This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which re",
        "This paper provides a principled and practical formulation for weight-sharing and quantization, using a simple mixture of Guassians on the weights, and stochastic variational inference. The main idea and results are presented clearly, along with illustrative side-experiments showing the properties of this method in practice. Also, the method is ill",
        "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional ",
        "This paper proposed an integration of memory network with reinforcement learning. The experimental data is simple, but the model is very interesting and relatively novel. There are some questions about the model:\n\n1. how does the model extend to the case with multiple variables in a single sentence?\n\n2. If the answer is out of vocabulary, how would",
        "This paper presents an interesting take on getting an ensemble for free whilst training a single network. However, your main accuracy comparison seems to exclude traditional ensemble methods, save for the very end of section 4.3, where the actual ensemble method you used is not mentioned. I would advise expanding this paragraph to explain what ense",
        "This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.\n\nThe idea of decoupling motion and content is interesting, and seems to wor",
        "This paper provides an interesting framework for handling semi-supervised RL problems, settings were one can interact with many MDPs drawn from some class, but where only a few have observable rewards; the agent then uses a policy derived from the labeled MDPs to estimate a reward function for the unlabeled MDPs. The approach is straightforward, an",
        "This paper proposes a new Bayesian neural network architecture for predicting the values of learning curves during the training of machine learning models. This is an exploratory paper, in that the ultimate goal is to use this method in a Bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictio",
        "\nThis papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks.\n\nAs pointed below, this is a useful addition.\n\nHo",
        "This paper describes a method to estimate likelihood scores for a range of models defined by a decoder.\n \n This work has some issues. The paper mainly applies existing ideas. As discussed on openreview, the isotropic Gaussian noise model used to create a model with a likelihood is questionable, and it's unclear how useful likelihoods are when model",
        "This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.\nThe group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Resu",
        "This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained.  Finally, all the weights are again jointly trained.  Experiments on a variety of image, text, and speech datasets demonstrate the approac",
        "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 ",
        "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key ",
        "This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. \n\nIntuitively, the vector-based gate working better than the sca",
        "This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.   \n\nPros:\n\n* This paper addresses an important question I and many others would have liked to know the",
        "This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy f",
        "This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and perf",
        "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so.",
        "This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. meth",
        "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. \n\nOverall, this is a well-written paper. \nAlth",
        "This paper fairly clearly presents a totally sensible idea. The details of the method presented in this paper are clearly preliminary, but is enough to illustrate a novel approach.",
        "This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.\n The paper is very clear and the authors have t",
        "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradien",
        "This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.\nI find that the insights provided in the paper, e.g. with respect to the effect of having a m",
        "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea ",
        "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just r",
        "This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in\nsequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition\nmatrices. It also generalizes the connections from lower ",
        "This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.\n\nTh",
        "This paper studies the problem of transferring solutions of existing tasks to tackle a novel task under the framework of reinforcement learning and identifies two important issues of avoiding negative transfer and being selective transfer. The proposed approach is based on a convex combination of existing solutions and the being-learned solution to",
        "This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the",
        "This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough \"hours\" of music, and where genre has been well constrained in order to allow for sufficient homogeneity",
        "This paper applies REINFORCE to learn MCMC proposals within the existing STOKE scheme for super-optimization. It's a neat paper, with interesting results.\n \n It's not clear whether interesting representations are learned, and the algorithms are not really new. However, it's a neat piece or work, that some ICLR reviewers found interesting, and could",
        "This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent inputs. The paper was clearly written, the approach is clever, and it's neat to see a practical algorithm driven by what is essentially a spiking netwo",
        "This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.\n\nThe paper is clearly written and is easy to follow.\n\nNovelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of",
        "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as w",
        "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.\n \n Where this paper really falls down is on originality. In particular, in the last two years there have been relat",
        "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed",
        "This paper first discusses a general framework for improving optimization of a complicated function using a series of approximations. If the series of approximations are well-behaved compared to the original function, the optimization can in principle be sped up. This is then connected to a particular formulation in which a neural network can behav",
        "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the author",
        "This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very na",
        "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a compa",
        "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learni",
        "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.\n \n Th",
        "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs. They also cover a variety of examples, including character level (PTB and Text8), word level (CNN questi",
        "This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performanc",
        "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. \nAs an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. \nBy predefining a set of operations, the model is able to learn the interface between",
        "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in ",
        "This paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex tas",
        "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most ",
        "This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.\n \n Pros:\n - ",
        "This paper discussses applying an information bottleneck to deep networks using a variational lower bound and reparameterization trick. The paper is well written and the examples are compelling. The paper can be improved with more convincing results on MNIST.",
        "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this",
        "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model\u2019s answer, which does not always come with a p",
        "This paper extends the GAN framework to allow for latent variables. The observed data set is expanded by drawing latent variables z from a conditional distribution q(z|x). The joint distribution on x,z is then modeled using a joint generator model p(x,z)=p(z)p(x|z).  Both q and p are then trained by trying to fool a discriminator. This constitutes ",
        "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the",
        "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at",
        "This paper studies the energy landscape of the loss function in neural networks.  It is generally clearly written and nicely provides intuitions for the results.  One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized.  It also quantifies, in a way, the degree of disconnec",
        "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- T",
        "This paper proposed a proximal (quasi-) Newton\u2019s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton\u2019s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the th",
        "This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to "
    ],
    "sampled": [
        "This paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nwikipedia). In particular, we address the problem of linking Wikipedia\nlinks in the open-web document wikipedia.en where every page consists of\nonly plain text rather than HTML tags and where there are many links\nthat are non-standard",
        "This paper presents several weakly supervised methods for developing NERs. The\nmethods aim to increase the recall of the annotators at the cost of\ndecreasing the precision of their work.\n\n# Keywords\n\n- Weak supervision\n- Sequence labeling\n- Named entity recognition\n\n# Introduction\n\nNamed entity recognition (NER) techniques are crucial for language\nprocessing tasks like question answering, machine",
        "This paper describes a rule based approach to time expression extraction. Its main objective is to provide a light-weight solution in Java and also work as a stand alone, so that it could be used to quickly validate a dataset and compare it with a gold standard dataset.\n\nYou have three options",
        "This paper presents a neural network-based framework for dialogue state\ntracking.\nThe main ingredients are a deep neural network with two distinct\noutputs -- a predicted dialogue state representation\n(typically extracted after applying the softmax nonlinearity to an\nembedding), and a predicted next-word choice (just an embedding).\nThe neural network",
        "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks.\nAn input sequence of symbols are embedded and fed into a sequence of\nBi-directional GRU layers. Then the input and the hidden vectors from each\nlayers are combined via element-wise summation to form the",
        "This paper presents a new dataset with annotations of products coming from\nonline cybercrime forums. The paper proposes an approach for building a\ndataset containing text and image product images to be used as a\ndataset for training a classifier for cybercrime product fraud\ndetection.\n\nThere are currently lack of data for product fraud detection in\nonline cybercrime forums. Due to",
        "This paper introduces UCCA as a target representation for semantic parsing and\nalso describes a technique which can be used to efficiently produce semantic parses from pre-computed logical forms. The proposed solution is described as a \"multi-stage\" semantic parser capable of dealing with semantic parsing problems with different structure and length requirements. We show an",
        "This paper proposed a macro discourse structure scheme. The authors carried out\na pilot study annotating a corpus consisting of 97 news articles from the\nDutch newspapers NRC Handelsblad and NRC Handelsdagen, in collaboration\nwith students. The annotators produced four separate analyses, describing\ntopics, topics and discussion moves, arguments and",
        "This paper proposes a framework for evaluation of word embeddings based on data\nefficiency and a novel method to detect outliers in word embeddings. The proposed\napproach is applied to the evaluation of word embeddings from two different\npublic sources (i.e., Glove and word2vec), on two distinct datasets, as well as\nword embedding produced during experiments. The framework shows",
        "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on many natural language processing\ntasks.  We first look at the architecture of the skip-gram model,\nintroducing four key architectural concepts, and then develop the\nformulation of the skip-gram model.  We then discuss two important\ntheoretical",
        "This paper presents a neural sequence-to-sequence model for encoding dialog\ncontext into a 1 dimensional vector. The encoder and decoder modules are based on the\nBidirectional recurrent Neural Network encoder-decoder (BRNN)\n([Sennrich et al., 2016](https://arxiv.org/abs/1609.08144)\nand\n[Bahdanau et al., 2017](https://arxiv.org/abs/1409.0473)\n).\n\nThe encoder includes a convolutional module that",
        "This paper compares different ways of inducing embeddings for the task of\npolarization\nprediction on datasets from the polarization-prediction benchmark.\n\nTo this end, the following methods are used:\n\n- **embeddings:**\n    These are the embeddings, i.e. the vector representations of the sentences.\n    A simple, untrained random word embedding is used",
        "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attentioning and\npooling. We found the proposed\nneural architecture to achieve state-of-the-art results in sixteen out of\nfifty-one semantic entailment-based evaluation datasets we investigated.\n\n![](assets/nn.png)\n\n> This repository has the code, test logs, and",
        "This paper proposes a supervised deep learning model for event factuality\nidentification.  The empirical results show that the model outperforms two state-of- the-art baseline machine learning models, one of them published in an IJCNLP proceeding.\n\nLink to the paper: https://arxiv.org/abs/1705.06667\n\nThis is a work in progress. Please feel free  to contact",
        "This paper presents the gated self-matching network for reading comprehension\nstyle question answering.\n\n# Table of Contents\n\n- [Introduction](#Introduction)\n- [Model architecture](#Model architecture)\n- [Comparison models](#Comparison models)\n- [References](#References)\n \n # Introduction\n  Reading comprehension questions (RCQs) are critical for machine comprehension,\nbut the task is challenging due to the complex interactions",
        "This paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively analyse the cold-start problem using\nsimulated data based on their real-world experience. We then\nemploy several popular features engineering methods to alleviate\nthe cold-start problem, and make use of data sparsity and label\nlack. Our experimental results verify that these methods can",
        "This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich (e.g. English) languages.\n\n## 1- Plain Translation of Nouns\n\nFirst, to begin with the most trivial of things, we need to do an ad-hoc\ntranslation: nouns. We will assume that the majority of",
        "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses multiple short-term memory networks to process its input,\nand predicts the final one-hot encoded target for the network training.\nIts goal is to help the model by providing additional internal\nstate representations and thus giving the network more freedom of action\nregarding",
        "This paper describes interesting and ambitious work: the automated conversion\nof Universal Dependency Trees to Binarization and NBE-like Treebanks. This work was done in the\ncontext of SemEval.\n\nThe results are not as good as I had hoped: the conversion fails almost all\nof the time and the parser (which is",
        "This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modularized deep attention-based architectures. Senses are learned using a multi-task\nmeta-learning framework, employing a modularized deep attention-based architectures for multi-task\nlearning and for sense identification. First, we show significant improvements in sense learning\nacross different tasks, including the identification of",
        "This paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an efficient online inference engine.\n\n# How to use this model\n\nThis is a \"state-of-the-art\" model which provides an interface for a\nCCG parser in TensorFlow. Currently there is only a CPU-based",
        "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to the song that was\nused as the reference for the lyrics generation.\n\nWe investigate two evaluation methods: the first one is the human evaluation of\nthe artist(s)/ song, using a predefined musician(s)/ song. This method",
        "This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization for text classification\nIn the NLP literature text classification algorithms use the frequency\nand location of words in a training set to distinguish between classes.\n\nThe challenge in text classification problems is the lack of discourse\nstructure",
        "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. Since we have both recurrent and\nattention networks, we can jointly optimize the parameters among them.\nThis joint optimization allows the two algorithms to cooperatively\nsolve the joint problem instead of having to choose one approach",
        "This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as building the dataset and preprocessing tools used to generate the data.\n\n## Architecture\n\nTo generate pictures, we use [SketchRNN](https://arxiv.org/abs/1704.03477)\n[6], which constructs a model that converts sketches to images.\nSketchRNN generates images in both grayscale and color, and also produces\nintermediate",
        "This paper propose a general framework for analyzing similarities and\ndifferences between the way in which computer programs are analyzed as compared to the way in which natural language is analyzed. In addition to describing how analysis differs between human languages and computer languages",
        "This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,\nSVD) to include ngram-ngram cooccurrence information, with the goal of improving sentence-level\nsemantic similarity, i.e. sentence similarity. To be more specific, we propose a method by\nfirst aggregating all ngrams in a given corpus to ngram-ngram cooccurrence probabilities\nfor further application in",
        "This paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relationships between them. These tasks are presented as simple\nexercises which students perform during workshop sessions. Participants were asked to answer\neither \u201cyes/no\u201d or to identify which word (of two) in the question constituted the most\n\u201cdistinguishing\u201d one. In",
        "This paper describes a straightforward extension to left-to-right beam search\nin order to handle arbitrary left-to-right orderings. In the previous paper on\nbeam search,\n_[Rico Sennrich](http://www.cis.uni-muenchen.de/~sennrich/ \"R. Sennrich's page.\")_ and _[Thomas Wolf](http://www.wolfsvillage.de/ \"Village of wolves.\")_ showed how to use an extra input stream to encode the token probability into a single number space that",
        "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence model, and uses the neural\nsequence to sequence to parse a semantic representation which is then used to construct\na query for a semantic search engine. The paper contains theoretical background\nand experiments. As a",
        "This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution method proposed in D. J. Weir et al. 1999^[http://www.cs.rochester.edu/~weir/papers/JLP-03.pdf]. This paper builds on previous work done on the problem of word sense disambiguation.\nIn particular, they showed that the lexical substitution method is effective, even",
        "This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the annotator:\n\n1. A teacher correcting assignments from her students. She can highlight the mistakes of her students.\n2. A scientist correcting the text of an article he has written.\n\n\n**Data:** corpus of about 60 essays which were",
        "This paper presents a dialogue agent where the belief tracker and the dialogue\nmanager are jointly trained with maximum likelihood (classical) gradient descent.\nWe show empirically its superiority over the method discussed in [10] for both \nhigh and low accuracy settings, indicating that both the classifier and the\ndialogue module are more stable",
        "This paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nSRL models. We use a state-of-the-art unsupervised pre-training\nscheme to learn deep structured representations of the input in a\nsequence-to-sequence manner. We use the pre-trained\nrepresentations to perform a sequence labelling task that can",
        "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something like a sequence of digits for human consumption. It\nfocuses mainly on binary sequences, but others are possible using common\nextensions and shifts.\n\n## Preambles \n\nWe want to be able to express sequences (or, really, a stream) of digits,\nand encode that",
        "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2018)\n\n\n### [Kadlec et al. (2018) -- Attention Sum Reader](https://openreview.net/pdf?id=BJl87y4Rb)\n\nTheir main system is trained with RL on an extremely large amount of text. There they train with the idea, that the sum",
        "This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the method works.\nFirst, a unigram encoder based on N-gram is introduced and evaluated. In\norder to improve encoder performance in this paper, a self-attention\nencoder has been introduced, which consists of four components. Then,",
        "This paper continues the line of work for applying word embeddings for the\nproblem of unsupervised morphological segmentation (e.g. Soricut & Och, 2017),\nby introducing a new class of algorithms that rely on the\ncharacteristic of the distribution of word embeddings in\ntheir own sense vectors.\n\n# Introduction\n\nThe main contribution of the paper is the novel approach",
        "This paper proposes a simple attention-based RNN model for sequence to sequence task without attention vector, such as MT, ASR and so on. Nowadays, most of the recent papers use an attention model to learn the relative importance between target and source words.\n\nWe implemented the attention model (refer to",
        "This paper presents a graph-based approach for producing sense-disambiguated\nsynonym sets from a collection of synonymy-annotated corpora. The approach has two major substeps. First, the approach computes candidate sense sets, \nconsidering a collection of disambiguation annotations. Second, the approach filters each candidate sense set by removing\nredundant",
        "This paper proposes a joint model of salient phrase selection and discourse\nrelation prediction in spoken meeting minutes by applying a standard machine\nlearning technique from sentence classification to discourse segments in\nspeech files, and then by selecting the highest scoring segment among the\ndiscourse segments. The proposed solution can help identify the",
        "This paper proposes a method for building dialogue agents involved in a\nsymmetric collaborative task, in which the agents receive a task to complete,\nand they work together to find a solution. We consider a domain in which\nthe environment generates some of the dialogue and the agents provide\nthe rest.\n\n[![](/img/icml.png)](https://arxiv.org/abs/1711.05106)\n\n##### Abstract (translated by Google)\n\nWe consider a domain in which",
        "This paper proposes an approach for classifying literal and metaphoric\nadjective-noun pairs. The authors create a set of adjective-noun\npair similarities based on structural rules and similarities to metaphors\n(similarity measures similar to those described in [Boruta2008] and\n[Chang2007]), and then classify whether a pair of adjectives is\nliteral or metaphoric based on those",
        "This paper proposes a novel strategy for zero-resource translation where\n(source, pivot) and (pivot, target) parallel corpora are available. In\naddition to the baseline-BTE, we propose BTE for the whole system, thus\ngetting rid of the pivot language while still maintaining high\naccuracy. Our strategy is also inspired by some recent work",
        "This paper considers the problem of KB completion and proposes ITransF for this\npurpose, which is a type of knowledge-graph completion. In fact, KG completion\nis related to the problem of extracting concepts from the knowledge base in\nits original form, which is a well-known NLP problem, but it can provide\nuseful information to",
        "This paper presents a method for translating natural language descriptions into\nedges or rules for specifying a graph. In particular, the\ndescriptions can be\n\n  a. *graphlets*, which are subgraphs that satisfy a set of\n     constraints. The method is adapted to support the translation\n    ",
        "This paper focuses on interpreting sarcasm written in Twitter identifying\nsentiment words and then using a machine translation engine to find an\nequivalent not sarcastic text. The idea is that if a sarcasm engine\nuses only the same words for sarcasm then it's easy for us, as people, to\nunderstand what the content of a message",
        "This paper describes a system to assist written test scoring.\n\n- Strengths:\nThe paper represents an application of an interesting method to a real-life problem\n\n- Weaknesses:\nThe paper is unclear in its description of the method and system so it is difficult to understand.\n\n- Contribution:\nThe paper applies a previously unpublished machine learning method to",
        "This paper introduces new configurations and training objectives that aim to learn embeddings of words from\n * the source language as well as the target language. These are combined with a translation objective\n * to predict target words conditioned by the source side of the sentence to",
        "This paper proposed a new phrasal RNN architecture for sequence to sequence\ngeneration. They have evaluated their architecture based on (i) the language modeling\n[1], (ii) speech synthesis [2] , (iii) translation [4], (iv) machine\ntranslation [3] and the image captioning. They observed strong performance on the\nall of the three",
        "This paper proposes new prediction models for Japanese SRL task by adopting the\nEnglish state-of-the-art NER and relation extraction systems. This new system\nscores higher than previous state-of-the-art systems using only Japanese data so\nit will benefit both JNLP group and Japanese language processing researchers.\nThe systems are built using\nconvolutional Neural Networks (CNN) trained",
        "This paper develops an LSTM-based model for classifying the sentiment of movie reviews. I\u2019ll use the reviews from this dataset: http://ai.stanford.edu/~amaas/data/sentiment/\n\n\n### The Basics Of An LSTM\n- **Input Vector: What\u2019s The Input To Become An LSTM.** \u2014 The standard way to use an RNN is to feed a sequence of",
        "This paper proposes a method for recognizing lexical entailment (specifically,\nhypernymy) in wordnet. The proposed method is based on two steps. First,\nthe algorithm identifies all nouns in a given domain that have a\ndescriptive role and an object. Second, it identifies all predicates\nthat",
        "This paper proposes a method for evaluating the quality of image-based and map-based reconstruction products, based on comparing the image of a real-world object with a reconstruction product. The method assesses the amount of structure loss as well as the quality of texture mapping. It uses",
        "This paper proposed a very interesting idea of using cognitive features for\nsentiment analysis and sarcasm detection. For this, a new preprocessing technique based on cognitive features of text was proposed. The\ntechnique consists of following 6 steps:\n\n- Segment the text to separate out sentences.\n- Tokenize each sentence in the segmented text. For",
        "This paper proposes a neural-styled topic model, extending the objective of\nword2vec to also learn from sentence and passages. We develop a topic model where each word in word2vec\u2019s input\nwords are connected to a cluster, and words in the same cluster represent a similar theme. From\nthe cluster we learn a distribution of",
        "This paper describes four methods of obtaining multilingual word embeddings and\na modified QVEC metric for evaluating the efficacy of such methods. In an\neffort to quantify the similarity between two words, we find that many\nmultilingual methods underestimate the similarity between words of different\nlanguages. We note this gap and compare the results",
        "This paper proposes a method for discovering correspondences between\nterms in two taxonomic data resources.  The result of this method can be used\nto provide information services for people who need to relate records in two\nsimilar data resources.  These information service can help users in\ndiscovering and matching new correspondences between the",
        "This paper proposes an approach for multi-lingual named entity recognition\nusing features from multiple languages by combining their feature representation. This would enable the system to extract the names of people and place of events such as births and deaths.\n\n## Contents\n\n\n1. [Introduction](#introduction)\n\n    (a) Problem Definition\n  ",
        "This paper proposes the new (to my knowledge) step of proposing to treat a\nnumber of sentence pair scoring method, that might involve text features (e.g.\nembeddings) and information on the sentence length, as a regression problem\napplicable to sentence pairs, rather than two separate classification tasks.\n\nThe question,",
        "This paper presents a transition-based graph parser able to cope with the rich\nrepresentations of a semantically meaningful sentence. The parser has been designed to solve two\nproblem in Natural Language Processing. First is to recognize if a given sentence has a gramatical\nerror, or simply to recognize the",
        "This paper presents a Stack LSTM parser based on the work of Henderson et al.\n(2008, 2013) on joint syntactic/semantic parsing with semantic role labeling, implemented as a deep neural network. Our parser\nhas a multilayered stack layer to form parse trees (instead of a linear stack as in many other SRL/SST parsers) and can thus\nhandle more",
        "This paper investigates three simple weight-pruning techniques for NMT, and\nevaluates their respective performance.\n\n# Introduction\n\nWith deep learning models, there has been a growth in the usage of transfer learning\nto pre-train language models. While these models are a huge leap forward for\nhumanity and modern NLP, there is still a lot",
        "This paper presents results on the UD treebanks to test delexicalized transfer\nparsers and an unsupervised parser which is enriched with external\nprobabilities. We used the treebanks annotated with part-of-speech,\nmorphological tag and deprel labels, but not headword annotation labels, for\nmost of the experiments, which is consistent with the results presented\nin our",
        "This paper presents an approach to tag word senses with temporal information\n(semantime \u2013 the name I\u2019ve chosen) as defined by: Hertzman, L.,\nMayer, E., & Shafir, E. (2011). Sensing time across languages:\nA case study in Hebrew and English. Language\nand Religion in America, 26(1), 141-72.\nThe idea is to allow a learner to",
        "This paper models event linking using CNNs. Given event pairs as inputs, the model learns to find the most common feature space that is shared between the two.\n\nEvent links are defined as:\n\n$$\n\\begin{align}\n\\theta_1 &= -\\log \\left [ \\frac{\\mathrm{L}_{\\boldsymbol{x}_1 | \\boldsymbol{y}_1}}{\\mathrm{L}_{\\boldsymbol{x}_1 | \\mu_1}} \\right ] \\cr\n\\theta_2 &= -\\log",
        "This paper describes a new deterministic dependency parsing algorithm and evaluates its correctness and efficiency on Chinese dependency parse data. It employs an algorithm based on a 1+1 parsing (Bradbury et al., 1989), i.e. all words are determined as non-terminals and all dependencies as non-projective edges. The algorithm has both guaranteed",
        "This paper improves significantly on the current state of the art by performing the training in stages and using regularisation to improve the training.\nWhen training in stages, I found that early training improves the test set performance, but that this benefit was lost if the training proceeded too far. It appeared that the neural",
        "This paper extends an approach to rate-distortion optimization to deep encoders [10]. We introduce a variant to the scheme knowns as \u2018distortion-level rate-distortion\u2019 (DLRD) [11], which introduces a mechanism to reduce the distortion over a group of frames that are known to be closer than the current",
        "This paper describes a new approach to meta learning by interpreting the SparseCoding model of Y. Cui and K. J. M\u00fcller in the context of a novel application. We investigate how to use the models learned for each source in an interpretable way so that we obtain a new method for meta learning. We use",
        "#ifndef __POINTS_H__\n#define __POINTS_H__\n\n#include \"cocosstudio/CCArmatureDefine.h\"\n#include \"cocos2d.h\"\n#include \"extensions/GUI/CCControlExtension/CCControl.h\"\n#include \"ui/UIWidget.h\"\n\nNS_CC_EXT_BEGIN\n\n\nclass Points",
        "This paper makes a valuable contribution to provide a more clear understanding of the structure and function of the brain. Our knowledge of brain anatomy and localization of neural signal transmission is only recently improved, providing the possibility of mapping the pathways of chemicals in the system to determine their relationship with function. This model can",
        "This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL algorithm in order to alleviate the bias problems inherent to deep RL and improve the performance of the algorithm in a wide range of scenarios.\n\nThe proposed solution is to add an unsupervised (self-supervised) auxiliary task to complement the learning algorithm (that is typically a",
        "This paper proposed to use RL and RNN to design the architecture of networks for 5G.\n\n[Poster](05.%20An%20Attention-Based%20End-to-End%20System%20for%20Resource%20Management%20of%205G%20Net.pdf)\n[Video](https://youtu.be/8Kb69hUaXnM)\n\n<br>\n\n\n### Faculty Name\n\nAmrutha Lakshmi, Suresh Babu<br>\nFaculty Name\n\n\n### Research Area\n\nComputer Communication.\n",
        "This paper details the approach that won the VizDoom competition at the 4th NeurIPS 2018 Deep Reinforcement Learning Workshop.  In summary, it uses a policy network to map image-based observations to continuous actions and then feeds the resulting output into a conventional value function network.  These networks are fed a batch",
        "This paper proposes learning on the fly an LSTM network which can model a sequence based model.\n\nIn order to make it more real I tried to train a sequence based model on random data which shows some random movement, it is supposed to show the pattern of movement\n\n# Usage\n\n### Installation\n\nThe requirements are in `requirements.txt` and are all python >=3.6, all",
        "This paper proposes to investigate attention loss and memory loss associated with the aging process in a rat model, using hippocampus and dentate gyrus neuronal activity, by comparing pre-aging, post-aging and post mortem rat models. The pre-aging model will consist of rats of 9-14 months, followed by",
        "This paper investigates the benefits of visual servoing using a learned\nvisual representation. The authors  propose a new servoing action which allows  the  robot to  actively  modulate the  orientation\nof the end effector by manipulating an artificial  representation  of  the ",
        "This paper proposes a nonparametric, maximum likelihood estimation approach which uses maximum a posteriori (MAP) estimates as prior guesses. \n\n## Example\n\nWe have a set of random vectors $y$ that arise from a linear transformation of a set of input vectors $X$.  Some of the output vectors are contaminated with noise to increase the difficulty of learning $y$ by",
        "This paper prunes entire groups of filters in CNN so that they see less input and can focus more on the important features. \n\nInception-v1:\n- Inception A,B,C : use 1x1 conv to reduce feature size.\n- Bottleneck : skip connection for spatial reduction.\n\nIn 1997 AlexNet: The first convolutional neural network to use ReLu nonlinear activations and data-driven dropconnect regularization.\n\nData",
        "This paper is well written, and well researched. It will allow students to put their knowledge of different types of government in the real world into practical application. A great resource for those who are thinking about becoming a caregiver.\nThis is a useful tool that would be very helpful in all",
        "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL): by using the trajectory-based, state-level skill-learninig (TLSL) framework, the RL agent is able to learn in sequential-decision-making. We present results for the MDP and for the sequential MDP shown in Figure 1 and 2 respectively, i.e. the MDP with feedback action-selection",
        "This paper proposes a model to learn across different views.\n\n### Data\n\nData was provided by Google.\n\n### Goal\n\nOur goal is to detect the relationship between\n\n$$\n\\begin{align}\nx &= \\text{dangerous weather condition} \\\\\ny &= \\text{disaster outcome}\n\\end{align}\n$$\n\n### Model\n\nThe models are simple feed forward networks.\n\n#### Compressed\n\nWe predict if an input is compressed with the following model.\n\n$$\nf(\\textbf{x}) = \\frac{1}{1 + e^{-\\textbf{wx} + b}}\n\\text{ (sigmoid)}\n$$\n\nThere's two learnable parameters $\\textbf{w}, b\\",
        "This paper proposed a neural attention model which has a learnable and adaptive set of receptive fields which respond to the image content. As the attention model is trained iteratively, it becomes more sensitive in areas it classifies, and less sensitive in areas which are not correctly classified. A demonstration for the method is performed in the",
        "This paper presents an information theoretic framework and demonstrates the applicability of a specialized technique for efficient sampling from a wide variety of low-dimensional statistical models. The framework is based on conditional independence assumptions that capture structural features of these general models. The specialized technique (a low-rank approximation method) enables",
        "This course reviews the historical development of theories of interpersonal relationships in a psychodynamically oriented framework. A specialization in Psychoanalysis provides a strong background for research in interpersonal processes. The specialization also requires significant mastery of psychoanalytic theory. The seminar course will explore the history and current relevance of the different theoretical approaches.",
        "This paper not only shows that a cache model on top of a pre-trained RNN can improve model performance on the task of identifying whether a speaker is \"aggressive\" in terms of expressing anger, but also why that works on an intuitive leve\n\n  * The question is phrased differently: Given a pre-trained sequence model (ex. an RNN) and a user who",
        "This paper proposed a novel adversarial framework to train a model from demonstrations of actions taken after the true state in the environment and the corresponding rewards, called state-action adversarial training (SA-AT). It addresses the limitation of some existing methods: using adversarial examples directly to train agents, or requiring the learning to be data-assisted",
        "This paper combines variational RNN (VRNN) and domain adversarial networks (DAN) to improve domain invariance. In particular, domain invariance of VRNNs is achieved through domain adversarial training (DANet). The paper was accepted to TCSVT 2016. \n\n**Sources**\n* code: https://github.com/dmitrykorostitsyn/dan-vrnn\n* slides from TCSVT 2019 https://github.com/dmitrykorostitsyn/dan-vrnn/blob/master/slides_TCSVT2019.pdf ([preferred](https://www.overleaf.com/viewer/345bd861f1a4228c0",
        "This paper introduces an innovative new theory of the development of cognitive, social, and emotional skills through the prism of early child development, play, and peer relationships, and explores its application to how we design and evaluate early childhood learning environments. From these ideas about how infants learn and develop through their first five",
        "This paper provides a principled and practical formulation of the main properties of probabilistic inference and their implementation: a posterior distribution is a weighted sum of the results of many posterior probabilities. Inference proceeds by evaluating the posterior of an observation in an iterative procedure that involves a maximum-a-posteriori (MAP)",
        "This paper provides an extensive description of all parts of the tool, as used in the evaluation conducted in \u201cAn Exploration of the Effects of a Collaborative Web-Based Model on Student Learning.\u201d\n\n![screenshot4small](images/screenshot4small.gif)\n\n## Introduction\n\n\n### Introduction: Aims \nThe principal aim in developing HERMES is to design",
        "This paper proposed an integration of memory network with reinforcement learning. However, many researchers still do not understand or can not get what this concept really is to begin with, even after years of experience. I do not think this issue can be solved simply by reading one paper, although some readers can be referred to",
        "This paper presents an overview of the challenges and opportunities facing the healthcare industry as a result of the healthcare reform legislation passed by the U.S. Congress on the Affordable Care Act. As of October 24, 2013, this 406 page legislation was scheduled to go into effect the following year. The paper emphasizes the complexities and",
        "This paper introduces an approach for future frame prediction in videos by decoupling motion prediction from content prediction. In particular, in addition to motion prediction, we propose to use a two-stage video-level content recognition scheme on video frames to predict high-level temporal information, and propose a multi-level video summarization scheme",
        "This paper provides an interesting framework for handling semi-supervised RL research. To quote the paper:\n\n> In the field of robotics, for example, robotic systems operate in a\ncomplex open-environment with highly dynamic sensory, physical, and semantic environments. A\nsystem that works well in one environment might fail when put into an\nenvironment where the system operates poorly. This raises",
        "This paper proposes a new Bayesian method of inference for quantum systems which relies on a quantum operator in the Hamiltonian of the dynamics governing a system in an environment, and which integrates information about the state of each system by including auxiliary particles in the environment. So called quantum decoherence operators are applied to the environment.",
        "\nThis papers adds to the literature on learning optimizers/algorithms that has gained momentum the past\nfew years. The main idea is very simple and the paper has a fairly good review of\nthe literature.\n\nThe basic message, that I found most useful, is the concept of learning dynamics.\nFor most methods, it is true that if we have a",
        "This paper describes a method to estimate likelihood scores for a range of models (linear, logistic, poisson, lasso, and random fields) based on MCMC. In addition, the approach of \"nested\" variational inference (NVI), where we reparametrize one of our models to get an approximate posterior distribution by integrating over another model (\"nested\" w.r.t. to this model), is described.",
        "This paper presents new way for compressing data used to train machine learning models using different neural network architectures.\n<!--break-->\nA main use case for compressing data for machine learning applications is transferring them from a source host machine to destination for training. Due to the vast amount of space used by model weights, this process may",
        "This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  Convergence is assessed through the validation data.  If training is converged, test scores with unmodified hyperparameters are computed.  If unmodified performance is poor, a search loop begins.  Each iteration in the search loop involves two steps. ",
        "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough to be applied to any machine with a memory unit. The module is based on two-level memory mechanism where the working memory stores simple binary patterns and the long-term memory stores complex patterns. In order to",
        "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to any scenario and a review the research efforts on a novel methodology for training a large scale of deep convolutional neural networks on a single server using the cloud resources (efficiency, energy modeling, etc.). This review, based on the new",
        "This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on all three English language tasks \u2014 part-of-speech, coreference resolution, and named entity recognition \u2014 along with a small improvement on the named entity prediction task.\n\n\n# Requirements\n* pytorch>=0.4\n\n\n# Training of the models\n##",
        "This paper performs a very important service: exploring in a clear and systematic way the performance of various existing tools, with different implementations, in the presence of changes in their source code. \nWe will make the case of the open source tools RTL-SDR and Sigrok and of their implementations in this open source project that will follow",
        "This paper addresses one of the major shortcomings of generative adversarial networks - their lack of explicit, tractable priors. We present a variational autoencoder which achieves a far superior generalization error without any prior assumptions.\nGenerative adversarial networks (GA-Net)1,2 are generative models with fully differentiable loss, which allow adversarial attackers to perform adversarial examples generation on",
        "This paper presents a novel way of pruning filters from convolutional neural networks (CNN).\n\nIdeally, all neurons should have non-zero weights in their pathway (i.e., a neural network node should have a pathway to at least one other node in the same network layer). However, in practice, we often have large numbers of filters (e.g., 100,000",
        "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM.\n(i) Pointing to spans (i.e. pointer network):\n\nThis is to identify spans from the context that best maps to a question.\nIt has an RNN which represents \"representations\" of the context and spans.\nIt needs the inputs (representations of",
        "This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long range dependencies in the image. This algorithm is based on the work of Ose et al. \"Adaptive Reconstruction-by-Reconstruction Registration\". ICCV, 2011.\n\n<h1>Installation: </h1>\n\n<p>\nFor a quick installation, just download the zip and extract the folder\nin a folder of",
        "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe proposed neural network model features a high precision on the state-of-the-art models, a fair accuracy and low computational cost.\n\n\n`Kernel Pairing` is used to efficiently compute the similarity scores using the global embedding of the kernel",
        "#include <sys/param.h>\n#include <sys/kthread.h>\n\n#include <err.h>\n#include <errno.h>\n#include <stddef.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n\nstatic void test_error_callback(char const *msg);\n\nint\nmain(int argc, char *argv[])\n{\n\tint error = 0, opt, test;\n\tstruct kthread_args args;\n\tconst char *pattern = NULL;\n\tint run_count",
        "This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoder training fast; as fast as traditional encoder and decoder networks, if any code is included in the bottleneck network.\n\n* Minimal, but useful modifications include weight sharpening, training with low loss, or compression of the",
        "This paper tests zoneout against a variety of datasets - character level, word level, and padded sequences of varying word and character length on a simple language modeling task.\n\n## Environment\nPytorch `1.1.0`\n\nThe datasets used are the standard 20 newsgroups dataset and the Penn Treebank. They are downloaded automatically into the datasets folder by running",
        "This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors combine their own model with the recent breakthrough of the Wasserstein-based variational auto-encoders by van den Oord et al., which enables the latent representations to be",
        "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less expensive than) a convolutional layer.\n\n## Abstract\n\nThe paper describes a way of increasing the number of parameters in a neural network without introducing any non-linear activation functions, which can be done just by randomly initializing weights. \nThe",
        "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2019), and their success. They use Bayesian optimization to fit different hyperparameter sets at each of the inner training loops of their approach. We show how the hyperparameter set used in Hyperband can be learned via Bayesian optimization, and",
        "This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of recurrent neural networks (RNNs). This is in contrast to RNNs that are widely applied for modeling time series. To the best of our knowledge, QRNN improves the performance of time series prediction tasks",
        "This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been shown to be a very powerful method for improving the generalisability of a predictive model. In this case however, we do not have a model to optimise, but rather a policy whose actions we optimise. This paper proposes an",
        "This paper studies the problem of transferring solutions of linear ordinary differential equations (ODEs)\nfrom one computational domain to another in the context of unsteady physics simulations.\nAs such it can serve as a companion paper for the article by D. P. G. Thomas,\n[An Interpolation Method for Time-Dependent\nFields]({{ site.baseurl }}{% link _papers/Thomas_1992_125002.pdf %}).\nThis is",
        "This paper proposes an approach to learning word vector representations for character sequences and character sequences with character-level context.\nWord level vector representation\n\nword2vec\n\nhttp://www.slideshare.net/tuncerguclu/character-n-gram-embedding-for-handwritten-number-recognition\n\nhttp://nlp.stanford.edu/pubs/srivastava14char.pdf\n\nhttp://lara.epfl.ch/w/u/nsofinu/documents/papers/2014-EtAl-Character-N-Grams.pdf\n\nCharacter level vector representation\n\nhttp://arxiv.org/abs/1702.02413\n\nhttps://arxiv.org/",
        "This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been researched in a recent period. These techniques are used to identify, and categorise song-like sounds. Most of the recent applications have focused on music classification (i.e. identifying a particular artist), or to segment music into tracks and other sound",
        "This paper applies REINFORCE to learn MCMC proposals within the existing STOKE scheme for super-optimization. It's currently under review for a publication.\n\nIf you use this package, please cite the paper: \"REINFORCE within STOKE: A new approach to MCMC proposal learning\".\n\n# Getting Started\n\nRequirements:\n\n* Python 2.7\n* Numpy\n* [stoke](https://github.com/thedavidmeister/stoke)\n* [matplotlib](http://matplotlib.org/)\n\nIf your Python is version 2.7, use",
        "This paper presented a method of improving the efficiency of deep networks acting on many inputs by making them aware of the input locations.\n\nThey were building on ideas used in **contextual representations**.\n\nI'll assume you're at least somewhat familiar with **Convolutional Neural Networks (CNN)**, but note that many of the same techniques are useful when working with",
        "This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to a model where the likelihood is the joint density, in the following paper:\n\n> Higgins, J. M., & Simpson, R. P. (2018). The role of uncertainty in modeling. Handbook of Statistics in Economics and Finance, 2018(2),",
        "This paper focusses on attention for neural language modeling and has two parts:\n(i) a systematic comparison of several attention function families;\n(ii) a detailed description of attention in a large scale pre-trained model.\n\n## 1) Attention\n\nAttention is a system that allows an agent to focus on a subset of its perceptual environment. For example, a dog is",
        "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, or _features_ as I shall call them. In most applications, the observations of the _hidden states_ $\\mathbf{x}_t$ will be some kind of image/video frame, which will be converted to a discrete feature vector (which will be a sparse vector,",
        "This paper presents a variational inference based method for learning non-stationary latent variables in Gaussian mixtures which arise in applications such as unsupervised time series analysis and anomaly detection. In this article, a novel non-parametric formulation of the mixture model is proposed with a variational expectation maximization approach. The new model can",
        "This paper first discusses a general framework for improving optimization of a mixed-integer program that results in infeasibilities due to redundant constraint pairs by adding a new auxiliary variable, termed the relaxation value. By extension (Section 2), the proposed approach is shown to result in additional improvements for other convex functions in the objective",
        "This paper extends preceding works to create a mapping between the word embedding space of two languages. For example, the word \"apple\" in English can also be found in Romanian via the mapping.\n\nThis project is not intended to be a comprehensive translation dictionary. But to just give a general idea on how the mapping works.\n\n#",
        "This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses both temporal and local dependencies within a sequence. Two aspects of learning speech recognition systems are relevant to speech segmentation: (1) the temporal segmentation of a speech signal is closely related to finding the best segment matching a known speech pattern (e.g.",
        "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified account and proposes some amendments and omissions that could help fill in the gaps with improved clarity. A detailed outline of the structure is laid out nicely. This should be a useful resource for anyone who has already read and",
        "This paper shows how policy gradient and Q-Learning may be combined together, improving learning speed by orders of magnitude.\n\n\n## Preliminaries: Reproduce OpenAI baseline\n\n```julia\nusing ReinforcementLearning\nimport ReinforcementLearning: PGTrajectory, SARSATrajectory, AbstractTrajectory\n\nfunction demo_sarsa(;kwargs...)\n    env = PendulumEnv(;tangential_reward=false, theta_offset=0.8, seed=42, kwargs...)\n    rng = StableRNG(42)\n\n    agent = SARSA(\n     ",
        "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions to the approach (such as using the data directly instead of learning a conditional mixture model), and shows how to implement such an approach in software. \n\n**Wittawat Jitkrittum** - witti@cc.rutgers.edu\n\nhttps://github.com/wjitk/invertible-dense/\n\n**N.B.: This paper and code does not provide a full solution",
        "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done quite badly.\nThere is no need to use batch normalization on the residual connections.\nOnly BN is used on the first convolutional layers.\nThe authors did this out of necessity, because BN for every input feature map is",
        "This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing coefficients with a model selector.\n# MATLAB Code\nPlease feel free to use my code! The main results are displayed in \"Simulations\" directory as follows:\n* MAT files of parameters of the problem (parameters.mat, data.mat, models.mat)\n* PDF files showing results",
        "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. The model, which is very well suited to this specific question classification task, achieves state-of-the-art performance on the Quora Question Pairs dataset (65.21% on the test set).\n\n##### Abstract (translated by Google)\n\n\n##### URL\n[http://arxiv.org/abs/1711.07067](http://arxiv.org/abs/1711.07067)\n\n##### PDF\n[http://arxiv",
        "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original algorithm. However, the proposed solution, unlike the original method, requires one to synchronize all episodes across workers. Moreover, there is no guarantee that the current implementation will work better than the original algorithm, and even may deteriorate the performance. For",
        "This paper presents an approach to learn to generate programs. Instead of directly learning from human program input, one will first learn a high-level model of programs using a domain-specific language. These high-level models will be then used to learn low-level programs by adding some more abstract details (for example, variable types), with fine-tuning on programs of varying complexity",
        "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment in order to use these for a \nmodel-based search problem in the agent\u2019s policy. We show that this can be formulated as a \nconstrained nonlinear programming problem",
        "This paper describes a new way of variable computation, which uses a different number of units depending on the input data stream. The number of units of this type used is computed based on an analytical model for the input data stream. For 1D image data, the proposed method is based on a 1D convolution, and is called Convolution-Driven",
        "This paper will focus on the impact of new business models: the model of crowdsourcing and the model of \u2018doing business together\u2019 (DBT) on innovation and economic activity. These models are widely used in SMEs, especially in the field",
        "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair, \nX denotes the source model, and Y the noise model. The noisy channel model is P(x|y) = g*P(y|X)P(x|y)g*, where g*g* are convolutions of the Gaussian functions to avoid the convolution of delta-functions.\n\n### Probability distribution\n\nP(x=i|y) is proportional to the",
        "This paper builds on the work of Weston (2016), using End-to-end memory network for visual question answering presented in Bengio (2014) to answer the question: *where* (a location) are *cats* present in the *picture*?\n\n1. The dataset\n=================\n\nBengio (2014) presents MNIST-QA dataset, that contains 27,000 questions, each with three possible answers from a set of 1,000 possible locations.\n\nThe dataset can be found",
        "This paper extends the GAN framework to allow for latent variables. The observed data set is expanded by drawing latent variables z from a conditional distribution q(z|x). This allows for the possibility of a one-to-many mapping between x and z. The generative model P(x|z) is modified to include the influence of z into the likelihood computation.\n\n* The \"Generative",
        "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to engage in contextual reasoning in dialogue.\n- [Link to the paper]({{ site.baseurl }}{% link _papers/2017-emnlp-dialogue-contextual-reasoning-with-dialogue.pdf %})\n- [Link to the website](https://cs.stanford.edu/people/raghu/dialogue-contextual-reasoning/)\n\n<!-- <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FJ4R34WJbFU\" \n           ",
        "This paper proposes a design principle for computation blocks in convolutional neural networks. One of their crucial benefits is their potential to be easily reconfigured from sequential to non-sequential to parallel implementations using low-cost hardware components. \n\nWe refer to such parallel blocks as \u2018computation units\u2019. Such computation units have multiple inputs",
        "This paper studies the energy landscape of the loss function in neural networks.  It is generally assumed that in neural networks, the loss function is the softmax cross entropy function applied on a linear layer. \n\nFor reference, take a look at the original paper here: https://arxiv.org/abs/1706.03762\n\n\nIn this paper, we investigate the energy needed to achieve a certain output.",
        "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of times it has been chosen, so it can be repeated indefinitely.\n\n# Action repetition 101\n\nAn action is a function that is performed by the player. For example, a walking action or",
        "This paper proposed a proximal (quasi-) Newton\u2019s method to learn binary DNN. The main contribution is that we proposed a general framework for the step-size calculation using the first and second derivatives without assuming a model, i.e. it can be used to solve more problems. In this paper, we implemented a proximal",
        "This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing denoising loss to improve model training. Instead of training adversarial adversaries using GAN training (see section 4 in [Adversarial Training]()), we train a classifier using GAN training, and use"
    ]
}