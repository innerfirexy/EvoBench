{
    "original": [
        "This paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nWikipedia).  (Its title and introduction are a little overblown/misleading,\nsince there is a lot more to bridging text and knowledge than the EDL task, but\nEDL is a core part of the overall task nonetheles",
        "This paper presents several weakly supervised methods for developing NERs. The\nmethods rely on some form of projection from English into another language. The\noverall approach is not new and the individual methods proposed are\nimprovements of existing methods. For an ACL paper I would have expected more\nnovel approaches.\n\nOne of the contributions o",
        "This paper describes a rule based approach to time expression extraction. Its\nkey insights are time expressions typically are short and contain at least 1\ntime token. It first recognizes the time token through a combination of\ndictionary lookup, regular expression match with POS tagging information. It\nthen expands the time segment from either dire",
        "This paper presents a neural network-based framework for dialogue state\ntracking.\nThe main contribution of this work is on learning representations of user\nutterances, system outputs, and also ontology entries, all of which are based\non pre-trained word vectors.\nParticularly for the utterance representation, the authors compared two\ndifferent neura",
        "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of t",
        "This paper presents a new dataset with annotations of products coming from\nonline cybercrime forums. The paper is clear and well-written and the\nexperiments are good. Every hypothesis is tested and compared to each other.\n\nHowever, I do have some concerns about the paper:\n\n1. The authors took the liberty to change the font size and the line spacing",
        "This paper introduces UCCA as a target representation for semantic parsing and\nalso describes a quite successful transition-based parser for inference into\nthat representation. I liked this paper a lot. I believe there is a lot of\nvalue simply in the introduction of UCCA (not new, but I believe relatively new\nto this community), which has the poten",
        "This paper proposed a macro discourse structure scheme. The authors carried out\na pilot study annotating a corpus consisting of 97 news articles from Chinese\ntreebank 8.0. They then built a model to recognize the primary-secondary\nrelations and 5 discourse relations (joint, elaboration, sequence, background,\ncause-result) in this corpus.\n\nThe paper",
        "This paper proposes a framework for evaluation of word embeddings based on data\nefficiency and simple supervised tasks. The main motivation is that word\nembeddings are generally used in a transfer learning setting, where evaluation\nis done based on how faster is to train a target model. The approach uses a set\nof simple tasks evaluated in a supervi",
        "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on the analogy task and for the general\nsuperiority of additive composition models. It also establishes a link between\nskip-gram and Sufficient Dimensionality Reduction.\n\nI liked the focus of this paper on explaining the properties of sk",
        "This paper presents a neural sequence-to-sequence model for encoding dialog\ncontexts followed by decoding system responses in open-domain conversations.\nThe authors introduced conditional variational autoencoder (CVAE) which is a\ndeep neural network-based generative model to learn the latent variables for\ndescribing responses conditioning dialog co",
        "This paper compares different ways of inducing embeddings for the task of\npolarity classification. The authors focus on different types of corpora and\nfind that not necessarily the largest corpus provides the most appropriate\nembeddings for their particular task but it is more effective to consider a\ncorpus (or subcorpus) in which a higher concentr",
        "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attention-based\nmatching, and aggregation. The model has two variants, one based on TreeRNNs\nand the other based on sequential BiLSTMs. The sequential model outperforms all\npublished results, and an ensemble with the",
        "This paper proposes a supervised deep learning model for event factuality\nidentification.  The empirical results show that the model outperforms\nstate-of-the-art systems on the FactBank corpus, particularly in three classes\n(CT-, PR+ and PS+).  The main contribution of the paper is the proposal of an\nattention-based two-step deep neural model for e",
        "This paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurre",
        "This paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively and quantitatively analyze the cold-start problem.\nThey observe that there is no enough prior data from a new user in this\nrealistic scenario. The traditional features fail to help to identify review\nspam. Instead, they turn to rely on the abund",
        "This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich ones (e.g.\nSpanish) in a two-step process. First, a system translates into a simplified\nversion of the target language. Second, a system chooses morphological features\nfor each generated target word, and inflects t",
        "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the late",
        "This paper describes interesting and ambitious work: the automated conversion\nof Universal Dependency grammar structures into [what the paper calls] semantic\nlogical form representations.  In essence, each UD construct is assigned a\ntarget construction in logical form, and a procedure is defined to effect the\nconversion, working \u2018inside-out\u2019 using ",
        "This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modular sense selection and representation process. The learning is\nachieved by a message passing scheme between the two modules that is cast as a\nreinforcement learning problem by the authors.\n\n- Strengths:\n\nThe paper is generally well written, presents most of",
        "This paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an efficient A* decoding algorithm.\nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more\nexpressive global parsing model, presumably because this factorization makes\nlearning easier. It's great that they also repor",
        "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to an artist whilst being\nfluent and co-herent. The paper is well written and the motivation for the\nmetrics are well explained.  \n\nThe authors describe both hand annotated metrics",
        "This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization. A RNN with attention\nmechanism is employed to compute a representation of text. The experiments on\nvarious of dataset shows the effectiveness of the proposed method. Below are my\ncomments:\n\n(1) From Table 2, it shows t",
        "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone ",
        "This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as corresponding logical forms and language\ndescriptions.\nThe goal seems to be to have a method where the complexity of pictures and\ncorresponding desciptions can be controlled and parametrized. \n\n - The biggest downside seems to be that the maxima",
        "This paper propose a general framework for analyzing similarities and\ndifferences in term meaning and representation in different contexts.\n\n- Strengths:\n* The framework proposed in this paper is generalizable and can be applied to\ndifferent applications, and accommodate difference notation of context,\ndifferent similarity functions, different type",
        "This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,\nSVD) to include ngram-ngram cooccurance statistics. To deal with the large\ncomputational costs of storing such expensive matrices, the authors propose an\nalgorithm that uses two different strategies to collect counts.  \n\n- Strengths:\n\n* The proposed work seems like a na",
        "This paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relations, similar to the\nanalogical reasoning task of Mikolov et al. (2013): Given an expression of the\nform \u201cX is for France what London is for the UK\u201d, X can be approximated by\nthe simple vector arithmetic operation L",
        "This paper describes a straightforward extension to left-to-right beam search\nin order to allow it to incorporate lexical constraints in the form of word\nsequences that must appear in MT output. This algorithm is shown to be\neffective for interactive translation and domain adaptation.\n\nAlthough the proposed extension is very simple, I think the pap",
        "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n\u201cprogrammer\u201d) which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a \u2018key variable\u2019 memory\ncomponent which stores (a) entities in the questio",
        "This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution task by using these inventories to filter\ncandidates. To do so, the authors first propose a metric to measure the mutual\nsubstitutability of sense inventories with human judgments for the lexsub task,\nand empirically measure the substitutabi",
        "This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the corpus:\n\n1) Student Revision Behavior Analysis and 2) Automatic Revision Identification\n\nThe latter is essentially a text classification task using an SVM classifier\nand a variety of features. The authors state that the corpus will be freely",
        "This paper presents a dialogue agent where the belief tracker and the dialogue\nmanager are jointly optimised using the reinforce algorithm. It learns from\ninteraction with a user simulator. There are two training phases. The first is\nan imitation learning phase where the system is initialised using supervising\nlearning from a rule-based model. Then",
        "This paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nstate-of-the-art system (Zhou and Xu, 2015) with recent best practices for\ninitialization and regularization in the deep learning literature.\nThe model gives a 10% relative error reduction which is a big gain o",
        "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something called the major system. In the major system, each digit\nis mapped to one or more characters representing consonantal phonemes; the\npossible mappings between digit and phoneme are predefined. The output of an\nencoding is typically a sequence of words co",
        "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop\nreasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but\nalso have major concerns to g",
        "This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the proposed model outperforms other\nbaselines if supervised data is available.\n\n- Strengths:\nThe paper is well-organized and easy to follow (the intuition of the proposed\nmethod is clear). It includes enough details to replicate experim",
        "This paper continues the line of work for applying word embeddings for the\nproblem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015;\n\u00dcst\u00fcn & Can, 2016). The proposed method, MORSE, applies a local optimization\nfor segmentation of each word, based on a set of orthographic and semantic\nrules and a few heuristic threshold values as",
        "This paper proposes a simple attention-based RNN model for generating SQL\nqueries from natural language without any intermediate representation. Towards\nthis end they employ a data augmentation approach where more data is\niteratively collected from crowd annotation, based on user feedback on how well\nthe SQL queries produced by the model do. Result",
        "This paper presents a graph-based approach for producing sense-disambiguated\nsynonym sets from a collection of undisambiguated synonym sets.  The authors\nevaluate their approach by inducing these synonym sets from Wiktionary and from\na collection of Russian dictionaries, and then comparing pairwise synonymy\nrelations (using precision, recall, and F",
        "This paper proposes a joint model of salient phrase selection and discourse\nrelation prediction in spoken meeting. Experiments using meeting corpora show\nthat the proposed model has higher performance than the SVM-based classifier.\n\n- Strengths:\nThe paper is written to be easy to read. Technical details are described fully,\nand high performance is ",
        "This paper proposes a method for building dialogue agents involved in a\nsymmetric collaborative task, in which the agents need to strategically\ncommunicate to achieve a common goal.  \n\nI do like this paper.  I am very interested in how much data-driven techniques\ncan be used for dialogue management.  However, I am concerned that the approach\nthat t",
        "This paper proposes an approach for classifying literal and metaphoric\nadjective-noun pairs. The authors create a word-context matrix for adjectives\nand nouns where each element of the matrix is the PMI score. They then use\ndifferent methods for selecting dimensions of this matrix to represent each\nnoun/adjective as a vector. The geometric properti",
        "This paper proposes a novel strategy for zero-resource translation where\n(source, pivot) and (pivot, target) parallel corpora are available. A teacher\nmodel for p(target|pivot) is first trained on the (pivot, target) corpus, then\na student model for p(target|source) is trained to minimize relative entropy\nwith respect to the teacher on the (source,",
        "This paper considers the problem of KB completion and proposes ITransF for this\npurpose. Unlike STransE that assigns each relation an independent matrix, this\npaper proposes to share the parameters between different relations. A model is\nproposed where a tensor D is constructed that contains various relational\nmatrices as its slices and a selection",
        "This paper presents a method for translating natural language descriptions into\nsource code via a model constrained by the grammar of the programming language\nof the source code.  I liked this paper - it's well written, addresses a hard\nand interesting problem by taking advantage of inherent constraints, and shows\nsignificant performance improvemen",
        "This paper focuses on interpreting sarcasm written in Twitter identifying\nsentiment words and then using a machine translation engine to find an\nequivalent not sarcastic tweet. \n\nEDIT: Thank you for your answers, I appreaciate it. I added one line commenting\nabout it.\n\n- Strengths:\n\nAmong the positive aspects of your work, I would like to mention t",
        "This paper describes a system to assist written test scoring.\n\n- Strengths:\nThe paper represents an application of an interesting NLP problem --\nrecognizing textual entailment -- to an important task -- written test scoring.\n\n- Weaknesses:\nThere isn't anything novel in the paper. It consist of an application of an\nexisting technology to a known pro",
        "This paper introduces new configurations and training objectives for neural\nsequence models in a multi-task setting. As the authors describe well, the\nmulti-task setting is important because some tasks have shared information\nand in some scenarios learning many tasks can improve overall performance.\n\nThe methods section is relatively clear and logi",
        "This paper proposed a new phrasal RNN architecture for sequence to sequence\ngeneration. They have evaluated their architecture based on (i) the language\nmodelling test evaluated on PTB and FBIS and (ii) Chinese-English machine\ntranslation task on NIST MT02-08 evaluation sets. The phrasal RNN (pRNN)\narchitecture is achieved by generating subnetworks",
        "This paper proposes new prediction models for Japanese SRL task by adopting the\nEnglish state-of-the-art model of (Zhou and Xu, 2015).\nThe authors also extend the model by applying the framework of Grid-RNNs in\norder to handle the interactions between the arguments of multiple predicates.\n\nThe evaluation is performed on the well-known benchmark dat",
        "This paper develops an LSTM-based model for classifying connective uses for\nwhether they indicate that a causal relation was intended. The guiding idea is\nthat the expression of causal relations is extremely diverse and thus not\namenable to syntactic treatment, and that the more abstract representations\ndelivered by neural models are therefore more",
        "This paper proposes a method for recognizing lexical entailment (specifically,\nhypernymy) in context. The proposed method represents each context by\naveraging, min-pooling, and max-pooling its word embeddings. These\nrepresentations are combined with the target word's embedding via element-wise\nmultiplication. The in-context representation of the le",
        "This paper proposes a method for evaluating topic quality based on using word\nembeddings to calculate similarity (either directly or indirectly via matrix\nfactorisation), achieving impressive results over standard datasets.\n\nThe proposed method represents a natural but important next step in the\nevolutionary path of research on topic evaluation. Th",
        "This paper proposed a very interesting idea of using cognitive features for\nsentiment analysis and sarcasm detection. More specifically, the eye-movement\npatterns of human annotators are recorded to derive a new set of features. The\nauthors claim that this is the first work to include cognitive features into\nthe NLP community. \n\nStrength: \n1. The p",
        "This paper proposes a neural-styled topic model, extending the objective of\nword2vec to also learn document embeddings, which it then constrains through\nsparsification, hence mimicking the output of a topic model.\n\nI really liked the model that the authors proposed, and found the examples\npresented by the authors to be highly promising. What was re",
        "This paper describes four methods of obtaining multilingual word embeddings and\na modified QVEC metric for evaluating the efficacy of these embeddings. The\nembedding methods are: \n\n(1) multiCluster : Uses a dictionary to map words to multilingual clusters.\nCluster embeddings are then obtained which serve as embeddings for the words\nthat reside in e",
        "This paper proposes a method for discovering correspondences between languages\nbased on MDL. The author model correspondences between words sharing the same\nmeaning in a number of Slavic languages. They develop codes for rules that\nmatch substrings in two or more languages and formulate an MDL objective that\nbalances the description of the model an",
        "This paper proposes an approach for multi-lingual named entity recognition\nusing features from Wikipedia. By relying on a cross-lingual Wikifier, it\nidentifies English Wikipedia articles for phrases in a target language and uses\nfeatures based on the wikipedia entry. Experiments show that this new feature\nhelps not only in the monolingual case, but",
        "This paper proposes the new (to my knowledge) step of proposing to treat a\nnumber of sentence pair scoring tasks (e.g. Answer Set Scoring, RTE,\nParaphrasing,\namong others) as instances of a more general task of understanding semantic\nrelations\nbetween two sentences. Furthermore, they investigate the potential of learning\ngenerally-\napplicable neura",
        "This paper presents a transition-based graph parser able to cope with the rich\nrepresentations of a semantico-cognitive annotation scheme, instantiated in the\nUCCA corpora. The authors start first by exposing what, according to them,\nshould cover a semantic-based annotation scheme: (i) being graph-based\n(possibility for a token/node of having multi",
        "This paper presents a Stack LSTM parser based on the work of Henderson et al.\n(2008, 2013) on joint syntactic/semantic transition-based parsing and Dyer et\nal. (2015) on stack LSTM syntactic parsing. The use of the transition system\nfrom the former and the stack LSTM from the latter shows interesting results\ncompared to the joint systems on the CoN",
        "This paper investigates three simple weight-pruning techniques for NMT, and\nshows that pruning weights based on magnitude works best, and that retraining\nafter pruning can recover original performance, even with fairly severe\npruning.\n\nThe main strength of paper is that the technique is very straightforward and\nthe results are good. It\u00e2\u0080\u0099s also cle",
        "This paper presents results on the UD treebanks to test delexicalized transfer\nparsers and an unsupervised parser which is enriched with external\nprobabilities.\n\nThe paper is interesting, but I think it could be improved further.\n\n(5.2) \"McDonald et al. (2011) presented 61.7% of averaged accuracy over 8\nlanguages. On the same languages, our transfe",
        "This paper presents an approach to tag word senses with temporal information\n(past, present, future or atemporal). They model the problem using a\ngraph-based semi-supervised classification algorithm that allows to combine\nitem specific information - such as the presence of some temporal indicators in\nthe glosses - and the structure of Wordnet -",
        "This paper models event linking using CNNs. Given event mentions, the authors\ngenerate vector representations based on word embeddings passed through a CNN\nand followed by max-pooling. They also concatenate the resulting\nrepresentations with several word embeddings around the mention. Together with\ncertain pairwise features, they produce a vector o",
        "This paper describes a new deterministic dependency parsing algorithm and\nanalyses its behaviour across a range of languages.\nThe core of the algorithm is a set of rules defining permitted dependencies\nbased on POS tags.\nThe algorithm starts by ranking words using a slightly biased PageRank over a\ngraph with edges defined by the permitted dependenc",
        "This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on r",
        "This paper extends an approach to rate-distortion optimization to deep encoders and decoders, and from a simple entropy encoding scheme to adaptive entropy coding. In addition, the paper discusses the approach\u2019s relationship to variational autoencoders.\n\nGiven that the approach to rate-distortion optimization has already been published, the novelty",
        "This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points.\n\nPros:\n\n- An interesting and feasible approach to me",
        "This paper was submitted to arXiv last week:\n\n",
        "This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. \n\nWith the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantl",
        "This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the ",
        "This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.\nThe pros of the paper are:\n1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting a",
        "This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-writ",
        "This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems ",
        "This paper proposes to investigate attention transfers between a teacher and a student network. \n\nAttention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.\nAuthors define several activat",
        "This paper investigates the benefits of visual servoing using a learned\nvisual representation. The authors  propose to first learn an action-conditional\nbilinear model of the visual features (obtained from a pre-trained VGG net) from\nwhich a policy can be derived using a linearization of the dynamics. A multi-scale,\nmulti-channel and locally-connec",
        "This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of s",
        "This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines. \nThe results are a 10% improvements in ResNet-like and ImageNe",
        "This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a metho",
        "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pix",
        "This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and Alex",
        "This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The ex",
        "This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the",
        "This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representation",
        "This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itsel",
        "This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different vi",
        "This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain a",
        "This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which re",
        "This paper provides a principled and practical formulation for weight-sharing and quantization, using a simple mixture of Guassians on the weights, and stochastic variational inference. The main idea and results are presented clearly, along with illustrative side-experiments showing the properties of this method in practice. Also, the method is ill",
        "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are",
        "This paper proposed an integration of memory network with reinforcement learning. The experimental data is simple, but the model is very interesting and relatively novel. There are some questions about the model:\n\n1. how does the model extend to the case with multiple variables in a single sentence?\n\n2. If the answer is out of vocabulary, how would",
        "This paper presents an interesting take on getting an ensemble for free whilst training a single network. However, your main accuracy comparison seems to exclude traditional ensemble methods, save for the very end of section 4.3, where the actual ensemble method you used is not mentioned. I would advise expanding this paragraph to explain what ense",
        "This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.\n\nThe idea of decoupling motion and content is interesting, and seems to wor",
        "This paper provides an interesting framework for handling semi-supervised RL problems, settings were one can interact with many MDPs drawn from some class, but where only a few have observable rewards; the agent then uses a policy derived from the labeled MDPs to estimate a reward function for the unlabeled MDPs. The approach is straightforward, an",
        "This paper proposes a new Bayesian neural network architecture for predicting the values of learning curves during the training of machine learning models. This is an exploratory paper, in that the ultimate goal is to use this method in a Bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictio",
        "\nThis papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks.\n\nAs pointed below, this is a useful addition.\n\nHo",
        "This paper describes a method to estimate likelihood scores for a range of models defined by a decoder.\n \n This work has some issues. The paper mainly applies existing ideas. As discussed on openreview, the isotropic Gaussian noise model used to create a model with a likelihood is questionable, and it's unclear how useful likelihoods are when model",
        "This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.\nThe group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Resu",
        "This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained.  Finally, all the weights are again jointly trained.  Experiments on a variety of image, text, and speech datasets demonstrate the approac",
        "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 ",
        "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key ",
        "This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. \n\nIntuitively, the vector-based gate working better than the sca",
        "This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.   \n\nPros:\n\n* This paper addresses an important question I and many others would have liked to know the",
        "This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy f",
        "This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and perf",
        "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so.",
        "This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. meth",
        "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. \n\nOverall, this is a well-written paper. \nAlth",
        "This paper fairly clearly presents a totally sensible idea. The details of the method presented in this paper are clearly preliminary, but is enough to illustrate a novel approach.",
        "This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.\n The paper is very clear and the authors have t",
        "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradien",
        "This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.\nI find that the insights provided in the paper, e.g. with respect to the effect of having a m",
        "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea ",
        "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just r",
        "This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in\nsequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition\nmatrices. It also generalizes the connections from lower ",
        "This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.\n\nTh",
        "This paper studies the problem of transferring solutions of existing tasks to tackle a novel task under the framework of reinforcement learning and identifies two important issues of avoiding negative transfer and being selective transfer. The proposed approach is based on a convex combination of existing solutions and the being-learned solution to",
        "This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the approach and experiments seem reasonable in terms of execution. The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented ",
        "This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough \"hours\" of music, and where genre has been well constrained in order to allow for sufficient homogeneity",
        "This paper applies REINFORCE to learn MCMC proposals within the existing STOKE scheme for super-optimization. It's a neat paper, with interesting results.\n \n It's not clear whether interesting representations are learned, and the algorithms are not really new. However, it's a neat piece or work, that some ICLR reviewers found interesting, and could",
        "This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent inputs. The paper was clearly written, the approach is clever, and it's neat to see a practical algorithm driven by what is essentially a spiking netwo",
        "This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.\n\nThe paper is clearly written and is easy to follow.\n\nNovelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of",
        "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as w",
        "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.\n \n Where this paper really falls down is on originality. In particular, in the last two years there have been relat",
        "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed",
        "This paper first discusses a general framework for improving optimization of a complicated function using a series of approximations. If the series of approximations are well-behaved compared to the original function, the optimization can in principle be sped up. This is then connected to a particular formulation in which a neural network can behav",
        "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the author",
        "This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very na",
        "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.\n\nIn terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a compa",
        "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learni",
        "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.\n \n Th",
        "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs. They also cover a variety of examples, including character level (PTB and Text8), word level (CNN questi",
        "This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performanc",
        "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. \nAs an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. \nBy predefining a set of operations, the model is able to learn the interface between the languag",
        "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in ",
        "This paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex tas",
        "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most ",
        "This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.\n \n Pros:\n - ",
        "This paper discussses applying an information bottleneck to deep networks using a variational lower bound and reparameterization trick. The paper is well written and the examples are compelling. The paper can be improved with more convincing results on MNIST.",
        "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this",
        "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model\u2019s answer, which does not always come with a p",
        "This paper extends the GAN framework to allow for latent variables. The observed data set is expanded by drawing latent variables z from a conditional distribution q(z|x). The joint distribution on x,z is then modeled using a joint generator model p(x,z)=p(z)p(x|z).  Both q and p are then trained by trying to fool a discriminator. This constitutes ",
        "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the",
        "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at",
        "This paper studies the energy landscape of the loss function in neural networks.  It is generally clearly written and nicely provides intuitions for the results.  One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized.  It also quantifies, in a way, the degree of disconnec",
        "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- T",
        "This paper proposed a proximal (quasi-) Newton\u2019s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton\u2019s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the th",
        "This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to "
    ],
    "sampled": [
        "This paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nwikipedia), with the goal of building a search engine in support of\ndomain experts that only provides results in their domain of expertise.\nWe present a model for representing the knowledge in Wikipedia that\nis well-suited to",
        "This paper presents several weakly supervised methods for developing NERs. The\nmethods\n\n### A Large-scale NLP Evaluation of Automatic Word Sense Induction\n\n\u2022 Computer Science\nACL\n\u2022 2020\nThis paper presents a comprehensive evaluation of WSI by comparing state-of-the-art WSI systems against the current standard for W SI, WordNet, and shows a simple hybrid system can",
        "This paper describes a rule based approach to time expression extraction. Its goal is to extract the temporal value and the unit from text in order to represent the start time and duration of activities. It was developed in the context of a Semantic Despatcher for meeting and event management systems,",
        "This paper presents a neural network-based framework for dialogue state\ntracking.\nThe main idea is to represent the current dialogue state as a hidden state of a recurrent\nneural network (RNN) so that the recurrent hidden state encodes both previous messages\nexchanged between the users and the dialogue",
        "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks. In this way, each sentence will be\nencrypted into a vector for later decoding tasks. We compare our approach on several datasets, including Twitter,\nYelp and StackExchange and also provide a detailed analysis",
        "This paper presents a new dataset with annotations of products coming from\nonline cybercrime forums. The paper also presents initial experiments with the aim of testing two state-ofthe- art detection methods for the automatic identification of counterfeit or suspect products of this kind. We analyze the potential of the application of both textual and visual analyses",
        "This paper introduces UCCA as a target representation for semantic parsing and\nalso describes a semantic parser that converts logical form and text into UCCA graphs. UCCA is a theory of\ndeep scene graph that can describe scene semantics, structure, and interactivity. One feature of the theory is that\na scene graph can be linearized to a",
        "This paper proposed a macro discourse structure scheme. The authors carried out\na pilot study annotating a corpus consisting of 97 news articles from the Chinese national daily\nGold Star Daily. The discourse structure of the articles was manually\nannotated . Experimental results show that inter-annotator agreement is reasonably",
        "This paper proposes a framework for evaluation of word embeddings based on data\nefficiency. The framework is based on the simple yet powerful idea of assigning an embedding and an unsupervised\nlearning task for which the embedding matches empirical data. This framework is motivated by several basic\nquestions in word embeddings. The proposed framework uncovers previously unrecognized relationships",
        "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success. We present a variant of hierarchical softmax that does not require any approximation, and is based on conditional probabilities. This leads to sig-ni cant improvements over the original skip-gram model. Both n\n\n### \u76f8\u5173\u5185\u5bb9\n\nThe mainstream",
        "This paper presents a neural sequence-to-sequence model for encoding dialog\ncontext and encoding its relevance to the user\u2019s goal. The model processes a dialog so\nthat the next utterance is sampled appropriately based on the goal, the previous turns of dialog, \nas well",
        "This paper compares different ways of inducing embeddings for the task of\npolarity classification.We report state-of-the-art results on the Semeval 2012 dataset. Our best embeddings were...\n\n2. #### Fast Prediction for the Regime Switching GARCH Model\n\nGao, Pinhui; Hansen, Peter R.; Lunde, Asger; Nason, James M.\nUsing a factorization involving the Fisher information matrix, we",
        "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, filtering and pool-ing the evidence.\n1\nIn this paper \u201ctextual entailment\u201d, \u201cNLI\u201d and \u201cargumentation\u201d used in\nthe context mean the same, therefore they are used interchangeably.\nISSN 1382\u20137857 International Journal of Social Robotics",
        "This paper proposes a supervised deep learning model for event factuality\nidentification.  The empirical results show that the model outperforms the state-of-the-art baseline on two public factuality datasets by a large margin.  We  investigate the effects of different neural network layer combinations on event factuality identification and also",
        "This paper presents the gated self-matching network for reading comprehension\nstyle question answering. In conventional reading comprehension datasets, each question\nis paired with a list of passages. The task that our model solves is to select a\nsingle correct passage from the list to answer the question.\nSuch datasets",
        "This paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively demonstrate that\ncold-start data is limited and that this data can help distinguish \nreview spam from non-spam. Then they create a dataset \ncomposed of cold-start reviews plus a small set of\nnon-spam reviews, and show that it is sufficient to\ntrain",
        "This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich languages\n(e.g. English) by utilising an intermediate state of the translation (a\nphrase-structure tree). This intermediate representation can be viewed as\nan explicit form of the output of an incremental transducer. We present\na",
        "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses f(x) and its derivative f \u0002(x) as synapses (weighting coefficients). For this embedding model, a training algorithm, which is an extension of a previously-proposed neural network method, is described. The extension adds... View full abstract\u00bb\n\n\u2022 ### A Survey",
        "This paper describes interesting and ambitious work: the automated conversion\nof Universal Dependencies treebanks into LTAGs. The conversion proceeds as follows: (1) For a\ngiven UD treebank, the syntactic tree is computed, together with\nsemantic, morphological and lexical annotations. (2) The resulting tree is then transformed into\nan LTAG. As a",
        "This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modular architecture. The novelty of the approach is a set of three components: 1) a\nmodular generative model of words, sentences and documents 2) a neural network\narchitecture for learning text representations in an unsupervised fashion and 3) a\nsense",
        "This paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an efficient implementation. In particular, the model\nuses a greedy decoder that prunes possible actions based on a future-cost estimation (FCE). Previous\nsystems that use an FCE score are inefficient and",
        "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to the original song,and to have a correct musical feeling.\n\\end{abstract}\n% use for special bibliography formatting\n%\\bibliographystyle{natbib}\n\\input{01-intro}\n\\input{02-rw}\n\\input{03-experiment_settings}\n\\input{04-eval_metric}\n\\input{05-results}\n\\input{06-conclusion}\n%\\printbibliography\n\\end{document} # -*- coding: utf8 -*-\nimport time\nfrom pymatgen.io.vasp.inputs import Incar\nfrom pymatgen.io.vasp import Poscar, Kpoints\n\n",
        "This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization. We conducted an exploratory experiment using six different sets of RST features of movie scripts in each of four different classes (romantic, suspense, and\nscience fiction, documentary). RST features were computed through two different",
        "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. CTC was introduced as a auxiliary training label. To further utilize CTC label in decoding stage, we use attentive decoding with attention and greedy-search strategy. Since CTC-label does not have information about word",
        "This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as a method to use those data to produce datasets of pictures which are in style of particular visual artists. Both processes require little to no human involvement. They exploit\nCategories: cs.CV # How many square roots does",
        "This paper propose a general framework for analyzing similarities and\ndifferences among time series. A multivariate model for seasonal time series is described, which allows for\ndynamic estimation of the seasonal period and for seasonal adjustments. In a univariate perspective our\nestimation procedure for seasonal period",
        "This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,\nSVD) to include ngram-ngram co-occurrence information and show that the resulting models perform better on downstream semantic and syntactic tasks, when compared to the original variants of these algorithms. Our best algorithms (Skip n-Gram & GloVe n-Grams) obtain state-of-the-art",
        "This paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relations in a sentence and semantic\nrelation extraction (SRE). A distributional vector for a word can be estimated either by\na corpus-derived semantic vector which captures word statistics in contexts outside the\nword\u2019s immediate context, or by an extrinsic",
        "This paper describes a straightforward extension to left-to-right beam search\nin order to incorporate a search over right-to-left transductions. This approach yields a\nmore practical implementation in many standardized speech recognition\nsystems.\n\n\\end{abstract}\n\n\n\\section{Introduction}\nThere are two primary reasons to consider incorporating right-context into speech\nrecognizers. First, right-context is likely to be available at test time, and\nso is",
        "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence transducer based on LSTM units. This is inspired by the\nsimilar approach used to address the challenge of syntactically conditioned generation. In this\nmethod, a large parallel corpus of surface forms paired with their",
        "This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution task, with the aim of improving the precision of disambiguation during word replacement by\nexplicitly incorporating conceptual information about the replaced word into the process.\nIn a second contribution we explore how these conceptual disambiguations influence",
        "This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the data set: \n\\begin{itemize}\n\\item We report the performance of baseline language models on the revisions. \n\\item We report the performance of several sentence-level revision markers for predicting revision markers such as \\textsc{insert}, \\textsc{delete}, and \\textsc{substitute}.",
        "This paper presents a dialogue agent where the belief tracker and the dialogue\nmanager are jointly trained to handle one-shot dialogues. The belief tracker uses the current turn\u2019s observation and \u03c1 previous unused system utterances as input and performs binary classification to predict the presence of user\u2019s hidden states. The dialogue",
        "This paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nwork presented in ACL-IJCNLP: Semantic Role Induction with a\nHybrid LSTM/CNN Representation. While this previous work\nachieved impressive performance on the CoNLL-2009 shared task,\nour new model achieves superior performance on both CoNLL-2009\nand CoNLL-2012",
        "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something like ordinary decimal notation, but with the property that no sequence ever encodes itself or\nany other sequence. The encoding's use of recursion in encoding the length prevents a circularity from happening when decoding, thus keeping\nthe set well defined,",
        "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) with gating to better capture the contextual dependencies, which results in both higher accuracy and lower perplexity. \nWe present in \\cref{subsec:gatedattention} the Gated Attention model and discuss its variants, including",
        "This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the proposed system is capable of generating relevant keyphrases in both of the domains. For a sentence like \\textit{China has its own foreign policy} the correct keyphrases would be \\textit{China} and \\textit{foreign",
        "This paper continues the line of work for applying word embeddings for the\nproblem of unsupervised morphological segmentation (e.g. Soricut & Och, 2003;\nZhang et al., 2010; Yatskar et al., 2014) in the direction of unifying and extending models based\non n-grams (e.g. Blunsom et al., 2015) with those exploiting the distributed nature of word\n(e.g.",
        "This paper proposes a simple attention-based RNN model for two widely studied natural language processing tasks: language modeling and question answering. It makes two possible contributions: First, it is an interesting hybrid of model architectures. It uses skip-connections and a Gated Recurrent Unit (GRU) as its recurrent unit. Second,",
        "This paper presents a graph-based approach for producing sense-disambiguated\nsynonym sets from a collection of word-sense-annotated corpus.\nOur approach starts with building a sense graph of synsets (similarity relationships between word senses) which is then used to identify sets of synonyms. The core contribution of the paper",
        "This paper proposes a joint model of salient phrase selection and discourse\nrelation prediction in spoken meeting summariza-\nCiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): Despite the growing interest in Automatic Speech Summarization, most summarization approaches rely on a separate recognition output as the input instead of using",
        "This paper proposes a method for building dialogue agents involved in a\nsymmetric collaborative task, in which the agents are given information and\nconstraints on their knowledge and reasoning capabilities, as well as a set of\nrules to guide their dialogue.  The method uses \na rule-based language to specify the constraints on the dialogue agents, and\nthen a neural",
        "This paper proposes an approach for classifying literal and metaphoric\nadjective-noun pairs. The authors create a corpus of over 600 metaphor-based analogies and train a neural natural language inference model using...\nArticle\nFull-text available\nBackground and aim The presence of cytotoxic lymphocytes is a well established marker of graft rejection after cardiac transplantation",
        "This paper proposes a novel strategy for zero-resource translation where\n(source, pivot) and (pivot, target) parallel corpora are available. Our method,\ncalled zero-resource neural machine translation (zr-nmt) utilizes the pivot\nlanguage in a similar way to pivoting in phrase-based machine translation, but\nwith the difference that it is performed in the neural",
        "This paper considers the problem of KB completion and proposes ITransF for this\npurpose. ITransF first projects sentences into the latent semantic space of a\nlanguage model to obtain representations of sentences as a set of vectors. Then it\nconstructs a semantic relation graph which is used to help interpret relations between\nknowledge. Finally,",
        "This paper presents a method for translating natural language descriptions into\n16:44\n\n### Difficulty prediction of test items in adaptive item bank\n\nDifficulty is an important indicator of test item quality, and therefore it is very important for the academic testing industry to predict the difficulty of test items in an effective and",
        "This paper focuses on interpreting sarcasm written in Twitter identifying\nsentiment words and then using a machine translation engine to find an\nequivalent not sarcastic but grammatically correct sentence. In Chapter 2,\nwe will focus on the problem of determining how much a sentence can express\nits meaning in a way that could be misunderstood. We",
        "This paper describes a system to assist written test scoring.\n\n- Strengths:\nThe paper represents an application of an interesting and useful technology.\n- Weaknesses:\nThe problem is clear (test scoring) and the solution is real but there is minimal technical novelty.\n- Experiments with a real dataset\n- Not enough details:\nI don't know the data and",
        "This paper introduces new configurations and training objectives for Transformer-based language models. 1) Instead of training language models from scratch, we construct an initial language model based on a pretrained English GPT-2 model. This better initializes 2) Language models should be trained on large and general datasets,",
        "This paper proposed a new phrasal RNN architecture for sequence to sequence\ngeneration. They have evaluated their architecture based on (i) the language ... for modeling syntax in an efficient way.\nConference Paper\nFull-text available\nThis paper proposes a novel algorithm to efficiently generate a large collection of unseen variations or",
        "This paper proposes new prediction models for Japanese SRL task by adopting the\nEnglish state-of-the-art universal multi-layer RNN (RNN) based model.\nWe combine multi-task learning\nand a method of pre-training on a language model (LM) which is based on the previous results of LM RNN models\nfor Japanese LM task and RNN based model",
        "This paper develops an LSTM-based model for classifying and forecasting time series data that contain a nonlinear trend component and are therefore not stationary. These models are commonly used to forecast economic and financial time series variables, including sales demand and price. To improve the LSTM-only models, a new",
        "This paper proposes a method for recognizing lexical entailment (specifically,\nhypernymy/hyponymy and meronymy/holonymy) from minimal textual descriptions. We view lexical...\nArticle\nFull-text available\nThis article investigates the emergence of naturalness in children's first- and second-learned languages during early language development. Specifically, we focus on 5-month-olds'",
        "This paper proposes a method for evaluating the quality of a user\u2019s mental model of a system. The proposed method is based on four different aspects. Constructive validity measures the ability to extract knowledge in order to reconstruct a scenario, while predictive validity is based on",
        "This paper proposed a very interesting idea of using cognitive features for\nsentiment analysis and sarcasm detection. The paper was not easy to understand by the authors because\nthey used a very complicated model to show the idea. Also, I did not understand all the techniques the authors used. This was not\ncompletely related",
        "This paper proposes a neural-styled topic model, extending the objective of\nword2vec to also learn the topic information from texts. The model, namely topic2vec, is trained on\nword2vec Skip-gram word2vec is a word embedding technique proposed by Mikolov et al. in 2013. It is an unsupervised learning algorithm, and in conjunction with the",
        "This paper describes four methods of obtaining multilingual word embeddings and\na modified QVEC metric for evaluating the efficacy of these methods. Our results indicate that (1) bilingual\nembeddings can significantly improve monolingual embeddings, (2) different methods of obtaining multilingual embedding\nperform differently under different metric conditions, and (3) there is a non-linear",
        "This paper proposes a method for discovering correspondences between different surfaces of the same object, from a single set of views captured from different viewpoints. A crucial step in our approach is detecting interest points in a robust manner. We utilize a robust variant of the recently introduced random sampling theory",
        "This paper proposes an approach for multi-lingual named entity recognition\nusing features from a set of mono-lingual datasets. \nThe motivation for this is that while  supervised training can lead \nto state-of-the-art results for mono-lingual datasets, it  can generalize less effectively to  multi-lingual datasets.\nOur approach uses cross-lingual named",
        "This paper proposes the new (to my knowledge) step of proposing to treat a\nnumber of sentence pair scoring methods at once by formulating a meta-problem\nof estimating the conditional probability of the entire sentence, a modeling\nchoice that we claim to be more consistent with the true",
        "This paper presents a transition-based graph parser able to cope with the rich\nrepresentations of a semantically-enriched multimodal ontology. We explore the use of several deep learning-based parsers\nin the context of the dialogue state tracking service of the Amazon Alexa platform.\nThe Amazon Alexa platform has a suite",
        "This paper presents a Stack LSTM parser based on the work of Henderson et al.\n(2008, 2013) on joint syntactic/semantic parsing\nand its application to semantic parsing\nof executable queries. We show a novel extension that captures\na richer lexicalized representation of predicates and semantics.\nOur models are trained with an incremental algorithm, which builds the\nrepresentations of predicates and actions",
        "This paper investigates three simple weight-pruning techniques for NMT, and\ntheir impact on the model accuracy and size. The three methods all fall out of a general\nframework of pruning weights with a score that is adjusted depending on the size of the associated\ninput feature vector. Given our three pruning",
        "This paper presents results on the UD treebanks to test delexicalized transfer\nparsers and an unsupervised parser which is enriched with external\nprobabilities. The results show that the unsupervised parser achieves\na competitive performance and is able to outperform the other methods for short\nsentences.\nSandeep Subramanian, Yufang Hou, Nikolaos Pappas, Philip Skopalik\n\n### Tutorial:",
        "This paper presents an approach to tag word senses with temporal information\n(05-SpainLinguistics/004, 07:06 UTC (Sat, 19 Mar 2016))\n@inproceedings{Sola-Escriva-etal:2008,\nSola-Escriva, M., G{\\'o}mez-Rodr{\\'\\i}guez, P. and Alonso, F. (2008).\nThe Effect of Temporal Adverbials on the Lexical Ambiguity of Selected Headwords.\nIn Proceedings of the First International Conference on Computational Creativity, IC3 2008, pp. 334-339.\n\\url{http://",
        "This paper models event linking using CNNs. Given event trigger words, a local context window around each word and a global sentence window are taken as inputs, a CNN is developed to automatically identify entities and their relationships. A maximum entropy model is used to determine",
        "This paper describes a new deterministic dependency parsing algorithm and its application in Java code size optimization. By replacing some fields in program state with deterministic inputs determined by a deterministic analysis, Java programs can be trimmed to a predetermined degree of size. Although there are usually a number of such",
        "This paper improves significantly on the algorithm presented in the previous paper: \"Combining the best of both: An MIMO detector that uses BP as a high-SNR algorithm, and sphere decoding as a low-SNR algorithm\". We present the LMMSE sphere decoder for the linearly modulated MIMO channel. The LMMSE SD does not use the symbol",
        "This paper extends an approach to rate-distortion optimization to deep encoders based on autoencoders and reinforcement learning.\nacl code_efficiency rd_on_the_fly 2020\nCode Efficiency in Deep-Learning Programs: An Empirical Study of Deep Convolutional Neural Networks\nACL\nSunil Gupta and David Alvarez-Melis and Tommi Jaakkola\nacl code_efficiency 2020\nEfficient Inference and Learning on Neural ODEs",
        "This paper describes a new approach to meta learning by interpreting the SdA as the building blocks of a recursive function sequence and leveraging this connection to derive a fast learning algorithm for SdA. The recurrent nature of SdA makes it ideal for representing algorithms and the resulting new meta learning algorithm can exploit this",
        "|X_{s_0}-X_{t_0}\\in \\cdot \\right]$ as $\\kappa_0:{\\mathbb R} \\to {\\mathbb",
        "This paper makes a valuable contribution to provide a more clear understanding of the physical meaning of the well known concept of the K-matrix in the study of nucleoli scattering. Our finding provides evidence that the K-matrix in the KS equations used to solve the Schrodinger-like KS equation can be interpreted physically as the probability amplitude",
        "This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL framework in an effort to improve its overall performance on several benchmark tasks. Our main contribution is to suggest a way of controlling the relative importance of each task by ...\n\n# The Rise of Artificial Stupidity\n\n15 Jul 2016\n\n## It has long been said that",
        "This paper proposed to use RL and RNN to design the architecture of networks for natural language understanding/process tasks. For natural language, we want to obtain the semantic representation of a s\u2026 How do you think RL could be integrated with deep learning? \u2022 LSTMs were originally designed to model time: they had gates that would allow information to be remembered or forgotten, they",
        "This paper details the approach that won the VizDoom competition in 2016 and 2017: deep reinforcement learning with deep Q-learning. The system was tested on a set of six environments that varied in terms of partial observability and complexity and for which the final scores were published. The results show that this approach",
        "This paper proposes learning on the fly for neural architectures at any design step to improve the final training performance by exploiting the rich information of the currently trained model. Second, we propose three progressive steps for model learning, namely training, pruning, and training again. This approach efficiently removes ineffective or redundant neuron (N)s at training time stage by pruning",
        "This paper proposes to investigate attention allocation in visual scenes with the help of spatial attention measures. For this purpose, a method of classifying images was developed based on two complementary measures that quantify spatial attention allocation. The two complementary measures are: (1) a measure based on",
        "This paper investigates the benefits of visual servoing using a learned\nvisual representation. The authors  propose to use a deep  neural network,  rather than  the  usual  hand  crafted visual features,  to provide a local visual representation of the current state of",
        "This paper proposes a nonparametric test of heteroskedasticity. This test is based on a combination of a kernel estimator of the squared ...\n\nD\u00e9posez votre fichier ici pour le d\u00e9placer vers cet enregistrement.\n\n## Multi angle\u00a0 Interactions of analytic number theory and geometry - lecture 1 Schindler, Damaris (Auteur de la Conf\u00e9rence) | CIRM (Editeur )\n\nA general introduction to the state",
        "This paper prunes entire groups of filters in CNN so that a light-weight CNN may be obtained while keeping the prediction performance acceptable. We start by observing that filters in each group generate spatial transformations of a particular convolutional channel which may be visualized. Filters are thus grouped into $k$\\ (spatial transformations) sets\\ each of size $m$\\",
        "This paper is well written, and well conceived, with few serious mis-steps. We commend the authors for their efforts in this field. The only comment that we would offer is that, even though the authors note that the inclusion of two-body channels do not affect the value of B, the analysis",
        "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL, which is a difficult transfer approach). By proposing a more modular setup with abstracted skill definition, we show that it is possible to transfer across different manipulation tasks. We use a meta-learning technique to improve this robustness and learn an algorithm",
        "This paper proposes a model to learn across different views of multiple views by capturing the high-level correlations between views and view-specific features jointly. As an important application example, this model is employed on classification task to identify forged images. We experimented various multi-layer feature representations from both original image and four feature extraction models on the publicly available CASIA face",
        "This paper proposed a neural attention model which has a learnable and soft alignment mechanism. The mechanism enables neural networks to directly learn and use spatial attention from the raw image pixels. We compared this model with a non-attention baseline on the task of referring expression comprehension in a machine comprehension style dataset. The experiments show the",
        "This paper presents an information theoretic framework for the classification problem, where multiple classification algorithms based on different loss (discrepancy) measures are compared on a common information theoretic ...\n\u2022 Lossless Compression schemes for the Binary TSM Model \ufeff\n\n(The University of Bergen, 2012-05-04)\nIn this thesis a scheme for lossless compression of",
        "This is an R Markdown document. To get started with a blank R Markdown document, select File > New File > R Markdown in RStudio. When you open a new R Markdown document in RStudio, a pop-up window appears that shows this code: From this format, you can easily convert text to any",
        "This paper not only shows that a cache model on top of a pre-trained RNN can improve learning efficiency, but also outlines a general procedure for achieving this, which in its minimal form is applicable to almost any type of network (not just RNNs). Moreover, the cache model allows us to do systematic fine-tuning and thus obtain substantially better performance than the",
        "This paper proposed a novel adversarial framework to train a model from demonstrations for tasks with different goal sets. Different from conventional imitation learning methods, we do not assume that there is a single solution and the learned model is generalizable to all goal states. Instead, we leverage the flexibility of GANs to support",
        "This paper combines variational RNN (VRNN) and domain adversarial networks for solving three problems of variational auto-encoder (VAE): i) how to model continuous stochastic processes for generating un-constrained natural pictures? ii) how to build in prior knowledge about the structure of the data? iii) how to estimate the probability of the model in a continuous latent",
        "This paper introduces and analyzes a new generalization of the classical problem of maximizing a concave function over a polygon, that we refer to as generalized polygon free concave utility maximization (gPCUM). In gPCUM, the utility index can be a continuous function if the polygon boundary is smooth, or it may be merely submodular",
        "This paper provides a principled and practical formulation of nonparametric inference for multidimensional spatial point process data. Our motivation is the analysis of counts of micro-organisms in space. We approach this problem by first reducing the spatial point process data to the sequence of its pairwise distances combined with the",
        "This paper presents, for the first time in the ...\n\u2022 325\n0 Downloads\nDOI: 10.1088/1742-6596/17/1/012110 ## Calculus 10th Edition\n\n$$\\lim\\limits_{x\\to-\\infty}\\sqrt{4x^2+x+1}=\\infty$$\nFor big values of $x$, $|x|\\gt0$. Hence, $$4x^2+x+1=(4x^2+\\frac{x}{2})+(1-\\frac{x}{2})=\\frac{1}{2}(8x^2+x)+(1-\\frac{x}{2})\\to\\",
        "This paper proposed an integration of memory network with reinforcement learning. A memory network is trained based on sample-based reinforcement learning algorithm and a novel attention mechanism. Besides, a novel deep reinforcement learning approach based on memory network (MNRL) is presented to solve the problem of robot decision making. The algorithm consists of four components. i)",
        "This paper presents an analysis of the problem of the accurate simulation of the motion of a spherical particle in liquid with a given translating velocity and an initial translational energy and rolling motion with a given rolling velocity and energy. The initial angular velocity of the particle is arbitrarily defined. The motion is simulated using",
        "This paper introduces an approach for future frame prediction in videos by decoupling motion estimation and context learning for future frame prediction. To this end, we propose an approach to model the motion with the motion representation, which is learnt from motion estimation between a pair of frames. Specifically, the",
        "This paper provides an interesting framework for handling semi-supervised RL using entropy regularisation. Semi-supervised learning (SSL) provides us with the tool necessary to make effective use of unlabeled data. One idea to build from this might be to combine SSL with RL, which given the success of deep learning based RL might enable efficient learning with",
        "This paper proposes a new Bayesian method for the modeling of longitudinal counts. It is designed to estimate both parameters of the marginal distribution (e.g., mean and variance) of the count, as well as those of the covariance within subjects, allowing to characterize the temporal dependence as well. For the latter, this paper focuses on the correlation",
        "\nThis papers adds to the literature on learning optimizers/algorithms that has gained significant momentum in recent years. \n\\cnote{There is good balance between these two review statements above, the former focusing primarily on the algorithms studied (a few are new in this article) whereas the latter focusing purely on the models studied.} %\n\\cnote{The article does not",
        "This paper describes a method to estimate likelihood scores for a range of models which are not necessarily nested but which share a common denominator. One approach common to many maximum likelihood inference procedures for nonnested models, for example, is to select a base model based on some ordering criterion and then to estimate the parameters of other",
        "This paper presents new way for compressing a digital signal. This approach is based on the idea that digital signals can be represented more efficiently using discrete Winer-Khintchine (WK) density function. Using WK approximation reduces a signal\u2019s rate while preserving its power and energy. Firstly, a signal should be approximated by WK pdf. It is",
        "This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  The weights of the layers are then fixed, and the layers' input sizes decreased by reducing the number of dimensions.  The last layer and some of the fully connected layers are replaced with pooling layers.  \n\nNext, the network",
        "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough to be combined with general function approximation architectures in reinforcement learning and transfer learning. We develop a framework for the memory module and a corresponding algorithm based on the framework, and apply the algorithm to a",
        "This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable across all fields of statistics. By transfer, we mean \u00e2\u0080\u0098domain adaptation\u00e2\u0080\u0099 by which an assumption is made that the source and target variables share some common information. Thus, transfer learning methods may also be applied to machine learning models under certain",
        "This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on several benchmarks. Word LSTM A longstanding problem with the Long Short Term Memory (LSTM) architecture is the brute-force learning of the gate connections for each task. Excessively many connections mean increased",
        "This paper performs a very important service: exploring in a clear and systematic way the performance of a multiobjective variant of Evolutionary Algorithms, the so-called Hypervolume Assisted Evolutionary Algorithms (HAEAs). Hypervolume Assistance AHAEA is a multiobjective evolutionary algorithm with control (GAEALA-X) that assists each individual during its evolution according to a progressive refinement of the hypervolume contribution",
        "This paper addresses one of the major shortcomings of generative adversarial networks - their lack of scalability and diversity in sample generation. Our solution is to introduce control codes into the generation process. These control codes are used to modulate the latent vector of the discriminator of a GAN in the hope that a wide",
        "This paper presents a novel way of pruning filters from convolutional neural networks without any decrease in classification accuracy. The weights of randomly chosen filters are set to 0, leading to a substantial decrease in computational cost for both training and testing. The performance is evaluated over the Caltech-256 and the ImageNet datasets using different",
        "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTMs (CMR + Ptr-Net) in the context of a multi-layer architecture. However, instead of training on SQuAD, we use a dataset that is more amenable to low-resource settings called Natural Questions (NQ) (Kwiatkowski et al., 2019).",
        "This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long range correlations by using a non-local operator. The modified model is then tested against the original one and also against two recent models that also take into account long range correlations. The proposed model is tested",
        "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe long document is regarded as a paragraph-level sentence. A single-layer attention mechanism \\cite{bahdanau2014neural} is proposed to automatically weight the paragraph-level sentences.\nBased on the text encoding of BERT \\cite{kenton2019bert}, the coattention mechanism extracts rich information from",
        "\u00a010.5194/acp-6-2459-2006, 2006.\n\nLiu, G., Li, J., Wang, Y., Wang, W., Ma, Y., Guo, H., and Chen, T.: Chemical characterizations of marine aerosols from different sources over Yangshan Island in summer,\u00a0J.\u00a0Environ.",
        "This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoder behave like an image codec. We argue that autoencoders have several key advantages compared to other approaches: 1) the minimization formulation lends natural support for adversarial training 2) the learned features can also be",
        "This paper tests zoneout against a variety of datasets - character level, word level, and pinyin level - and compares the modeling technique with more conventional RNNs and sequence-to-sequence models. We show that zoneout is a strong baseline that can serve as a drop-in replacement for many existing RNN-based models. Our experiments were",
        "This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors achieve their method by using a modified loss function as well as a weighting scheme.\n\n# Implementation details\n\nThis paper provides one example application of generating fake samples",
        "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) state-of-the-art models. We achieve this by making several independent paths inside a layer, each being of extremely small size, and train them in an alternating fashion. Our method, called Alpha-Net, enables us to increase the",
        "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS\u2019 15) that considers parallel Successive Halving. If, just like in distributed Hyperband, we try new configurations $$\\mathcal{C}_s$$ in each Successive Halving (rather than only in the final one), we get a procedure called Dist-HyperBand (D-HB). An implementation can be",
        "This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of recurrent neural networks (RNN) on limited hardware. The proposed network takes the best of recurrent neural networks (RNN) and feed forward networks by unlocking training weights with inactivation and delay. Moreover, the QRNN",
        "This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been proposed as an approach to reduce the variance inherent in the policy-gradient approach. However, the technique is generally heuristic and poorly understood. In this paper, we rigorously explore properties of ensemble policy-gradient training from a statistical learning point of",
        "This paper studies the problem of transferring solutions of optimization problems when the tasks are different. We propose an algorithm to efficiently perform this problem which is akin to multi-task learning. The algorithm leverages the information from both the tasks to find good solutions of the original problems. We prove we can",
        "This paper proposes an approach to learning word vector representations for character sequences and evaluates these representations for use in neural machine translation that we call sequence-to-sequence learning. Using these representations of character sequences, we can predict the probability that we have a particular sequence of words. The proposed approach builds representations of character sequences,",
        "This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been around for some ...\n285\n0.1143\nThe development of robotic technology has facilitated robotic surgeries, thereby bringing the potential of microsurgery closer to the clinical environment. However, robotic microsurgery may require extensive training due to the complex ...\n286\n\n287\n0.1186\n301\n\n288\n0.1194\n\nAugmented Reality (AR) technology has",
        "This paper applies REINFORCE to learn MCMC proposals within the existing STOKE scheme for super-optimization. It's clear from the abstract in \\cite{reinforce} that the idea is: ``We show how to use\nreinforcement learning to learn proposals for MCMC. Specifically, we apply REINFORCE to the recently proposed STOKE optimization framework, and show that both optimization",
        "This paper presented a method of improving the efficiency of deep networks acting on 3D point cloud data. \u6536\u8d77 \u2022 In this work we study the long time behavior of solutions of the evolution problem associated to nonlocal transport equations, namely the so-called transport by underground diffusion (TDUD) equation. Our aim is to analyze how the",
        "This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to modeling event data.\n Anush Tserunyan 12 hours ago\n #MeetMathTheory (@MeetMathTheory)\n1/2\n11/2013 - 5:20am\nHow can a math talk be boring?\nHere's how ...\nOne big hole I always see with the undergraduate math degree is that it doesn't give",
        "This paper focusses on attention for neural language modeling and has two main contributions. In the first part, we focus on how to incorporate the word-level attention mechanism proposed in \\cite{yang_hierarchical_2016} for language modeling. In the second part, we focus on how to incorporate attention for neural dialog modeling with a context aware decoder which",
        "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, nonlinear transitions, and partial latent state observation... More Presented by Dr. Hiroaki UCHIDA Type: Poster Session: Poster Session I Track: New theoretical developments, new physics analyses, and new tools We present a new algorithm to simulate relativistic quantum particles",
        "This paper presents a variational inference based method for learning nonlinear dynamics models under observational conditions for complex physical systems. The proposed method can learn dynamics models that are capable of generating realistic synthetic data, and can also be used for estimating uncertainty in the learned model of the underlying dynamics. The",
        "This paper first discusses a general framework for improving optimization of a stochastic simulation-based design, where the performance measure is defined with a risk criterion. This defines a so-called robust design problem, where the design objective is to find an optimal space-filling trajectory such that the uncertainty of the stochastic performance measure can be",
        "This paper extends preceding works to create a mapping between the word embedding space of two languages. To this end, we propose a novel method based on the concept of locality preserving projections. We experiment our algorithm using various sets of translation lexicons (from English to French) and we compare the obtained bilingual word embeddings",
        "This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses the problems of limited dictionary size for finite state generation, the need for high accuracy and the trade-off involved in these. Sequences are first decomposed into elementary units using a pre-processing step. Next, these elementary units are combined using a tree based",
        "This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified theory of the S-boxes, but does not present new insight or substantial improvement over the existing ones. 3:04:22-3:05:39 4 TIMING ATTACKS AND COUNTERMEASURES This paper is technically sound, but could be improved significantly by showing how to extend the existing",
        "This paper shows how policy gradient and Q-Learning may be combined together, improving learning efficiency on a system where the underlying states and rewards are unknown. 05. This means we just keep replaying the sample over and over again until it converges. The agent samples actions from this \u201creplay\u201d buffer instead of selecting actions from",
        "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and discusses future work in a systematic way. To this end, we review and expand on the existing literature that covers neural network construction and use in probabilistic learning (Section 2), explore a wide variety of latent-variable models (Section 3)",
        "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done badly due to high computational costs. We first define a Recurrent Batch Normalization for Recurrent Neural Networks, which does not suffer from the computational issues; thus making scaling up RNNs much easier. In addition, we",
        "This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing gradient norm. In the experiments we consider minimization of the classical Rosenbrock function with Levenberg-Marquardt from the 1960s for nonlinear least squares. We present theoretical and practical argument on what a good choice for a good function",
        "This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task that requires joint reasoning across multiple sentences: recognizing compositions of events in a chain of sentences (event chain recognition). For example, we want to predict that \u201cSteve buys a gallon of milk; and, while loading",
        "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original algorithm. If you have not gone through our '[Deep Q based RL]' mini-series then you should read (and understand) all the three posts first. I will continue with previous posts series, if you not already read them.\n\n## Dueling Deep Q",
        "This paper presents an approach to learn to generate programs. Instead of directly learning programs, it transforms programs to trees and extracts grammars from the trees for program generation. The approach works on three important steps: constructing a decision tree, learning a context-free grammar and using grammar-based tree-generation to solve the problem. The experiments and results in the paper",
        "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment and present a method of using their Bayesian model ensemble for policy search by optimizing the expectation of the action value function.\nExperimental results show that Bayesian model-based",
        "This paper describes a new way of variable computation, which uses a different number of units depending on the amount of activity.\ncited by\narXiv:1304.1946 [cs.CV] (2013)\nThe proposed approach utilises the principle of Hebbian reinforcement learning to adjust the network of interconnections between feature detectors. Hebbian learning incorporates both spike-timing and spike-count information from neurons in the network such that the",
        "This paper presents two analytical methods for calculating the magnetic force on a circular loop. These methods are based on using either the Biot-Savart law or equivalent surface elements to get the magnetic field distribution inside a circle carrying",
        "This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  { x_1 \\to \\cdots \\to x_L  } ,  { y_1 \\to \\cdots \\to y_M  } .\nThe input and output are sequence.  { x_1 \\to \\cdots \\to x_L  } is a input sequence:\n\n\n{\n\\sum^{L}_{i=1}",
        "This paper builds on the work of Weston (2016), using End-to-end memory network for question answering.\nitor and memory networks. This paper builds on the work of Weston\n\nC. Weston. \u201cEnd-to-end memory networks.\u201d In Proceedings of the 30th\nA C. Weston, S. Chopra, and A. Bordes. \u201cMemory networks.\u201d In Proceedings\nd on the work of Weston (2016), using End-to-end memory network for ques-\nA New",
        "This paper extends the GAN framework to allow for latent variables. The observed data set is expanded by drawing latent variables z from a conditional distribution q(z|x). Both discriminator and generator networks then receive additional inputs corresponding to these latent variables. To facilitate computation the posterior distribution, p(z|x) is also needed. It is approximated by the conditional distribution,",
        "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from dialogue. The task we specify requires that the agent solve a simple reasoning puzzle using a conversation where the dialogue system has to make inferences about the goal of the other agent, reason",
        "This paper proposes a design principle for computation blocks in convolutional neural networks (CNN) in order to reduce the hardware cost. To improve the efficiency of a CNN accelerator the computation block should operate on a small number of filters (typically one filter) and the total number of computations per data",
        "This paper studies the energy landscape of the loss function in neural networks.  It is generally  observed that certain models have very  high loss on   certain  points. On the other hand, it is  observed that such points do not have a large norm.  As a result, many   methods are",
        "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetition. This is simply written as a*x, and in the reward can be applied a larger or smaller \"energy bonus\" the bigger x is. This is an adaptation of the",
        "This paper proposed a proximal (quasi-) Newton\u2019s method to learn binary DNN. The main contribution in this paper included the following: (i) to learn DNN parameters from a constrained domain, the paper implemented the augmented Lagrangian method to transform the constrained (binary) problem into an unconstrained problem; (ii) to solve the unconstrained",
        "This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing denoising feature matching. The main difference between two papers is the detail of the method. In Improving Generative Adversarial Networks with Denoising Feature Matching, the feature is extracted by"
    ]
}